{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5d9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to read mdEditor files, and write out as shapefile\n",
    "#and shapefile with geography, if applicable\n",
    "#by Tamatha A. Patterson; verson 5; December 2022\n",
    "#distribution section updated.\n",
    "#extent merged with metadata and written to shapefile. #FeatureCollection type handled.\n",
    "#associations section updated.\n",
    "\n",
    "def mdEditor_read(metadataToRead, contact_md, csvname, workspace, recordtype = 'all'):\n",
    "    #recordtype used to write chosen resource types  \\\"all\\\", \\\"project\\\", etc.in future versions\n",
    "    #extent/geography read when is True, skipped when False.\n",
    "    import os\n",
    "    import json\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import collections\n",
    "    import fiona\n",
    "    from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
    "    from datetime import date\n",
    "    \n",
    "    def removeComma(string): #define remove comma function\n",
    "        return string.replace(\",\",\"; \")\n",
    "    \n",
    "    def listToString(s): #define converting list to a string format\n",
    "        str1 = \"; \"\n",
    "        return (str1.join(s))\n",
    "    \n",
    "    def def_value():\n",
    "        return \"none\"\n",
    "    \n",
    "    os.chdir(workspace) # assign working directory\n",
    "    today = date.today()\n",
    "    \n",
    "    df = pd.read_json(metadataToRead)#read JSON metadata files into dataframe\n",
    "    values = df.get('data') #assigns metadata to values\n",
    "    for e in range(0, len(values)): #json file may have multiple metadata records\n",
    "        element = values[e] #assign list value e containing the metadata #keys= id, attributes, type\n",
    "        ID = element.get('id') #get metadata id\n",
    "        attribute = element.get('attributes') # keys= profile, json, data-updated\n",
    "        typpe = element.get('type') #get metadata type\n",
    "        if typpe != 'records':  \n",
    "            # skip if metadata is not a record and is a data dictionary, setting, schemas, custom-profiles...\n",
    "            continue  #go to next record\n",
    "\n",
    "        #parse 'attribute' to create profile, json, and date-updated\\n\",\n",
    "        dateUpdate = attribute.get('date-updated') #create dateUpdate value--where does this date come from???\n",
    "        jsondata = attribute.get('json') #create'json' data value\n",
    "        profile = attribute.get('profile') #create 'profile' value\n",
    "        \n",
    "        #convert string to dictionary\n",
    "        jsondatadict = json.loads(jsondata)  #3 keys = schema, metadata, mdDictionary\n",
    "        schema = jsondatadict.get('schema')\n",
    "        metadata = jsondatadict.get('metadata') #4 keys = metadataInfo, resourceInfo, associatedResource, resourceDistribution\\n\",\n",
    "        mdDictionary = jsondatadict.get('mdDictionary')\n",
    "        #get metadata key entries\n",
    "        metadataInfo = metadata.get('metadataInfo') #6 Keys = metadataIdentifier, metadataContact, defaultMetadataLocale, metadataDate, parentMetadata, metadataStatus\n",
    "        resourceInfo = metadata.get('resourceInfo') #12 keys = resourceType, citation, pointOfContact, abstract, shortAbstract, status, defaultResourceLocale, extent, keyword, purpose, taxonomy, timePeriod\n",
    "        associatedResource = metadata.get('associatedResource') #is list\n",
    "        resourceDistribution = metadata.get('resourceDistribution') #keys = n\n",
    "\n",
    "        #parse metadataInfo dictionary 6 keys\n",
    "        metadataIdentifier = metadataInfo.get('metadataIdentifier') #Harvest as ID to fields\n",
    "        metadataContact = metadataInfo.get('metadataContact')\n",
    "        defaultMetadataLocale = metadataInfo.get('defaultMetadataLocale')\n",
    "        metadataDate = metadataInfo.get('metadataDate')\n",
    "        parentMetadata = metadataInfo.get('parentMetadata')\n",
    "        metadataStatus = metadataInfo.get('metadataStatus') #Harvest as status to fields\n",
    "\n",
    "        #get metadata uuid identifier; autocreated in mdEditor\\n\",\n",
    "        if metadataIdentifier['namespace'] == 'urn:uuid':\n",
    "                metaIdentifier = metadataIdentifier.get('identifier')\n",
    "\n",
    "        #parse resourceInfo 12 keys: 'resourceType', 'citation', 'pointOfContact', 'abstract', 'shortAbstract', 'status', \\n\",\n",
    "        #...'defaultResourceLocale', 'extent', 'keyword', 'purpose', 'taxonomy', 'timePeriod'\\n\",\n",
    "        resourceType = resourceInfo.get('resourceType')\n",
    "        citation = resourceInfo.get('citation')\n",
    "        pointOfContact = resourceInfo.get('pointOfContact')\n",
    "        abstract = removeComma(resourceInfo.get('abstract')) #harvested to fields\n",
    "        if resourceInfo.get('shortAbstract') == None:\n",
    "            shortAbstract = \" \"\n",
    "        else:\n",
    "            shortAbstract = removeComma(resourceInfo.get('shortAbstract'))#harvested as shortAbstract to fields\n",
    "        statusList = resourceInfo.get('status')\n",
    "        status = statusList[0]#harvest as status to fields\n",
    "        defaultResourceLocale = resourceInfo.get('defaultResourceLocale')\n",
    "        extent = resourceInfo.get('extent')\n",
    "        keyword = resourceInfo.get('keyword')\n",
    "        if resourceInfo.get('purpose') == None:\n",
    "            purpose = \" \"\n",
    "        else:\n",
    "            purpose = removeComma(resourceInfo.get('purpose')) #harvested to fields\n",
    "        taxonomy = resourceInfo.get('taxonomy')\n",
    "        timePeriod = resourceInfo.get('timePeriod')\n",
    "\n",
    "        #find last update date from metadataDate\n",
    "        #Consider comparing this to last run date and only reading metadata updated after????\n",
    "        if len(metadataDate) == 1:\n",
    "            lastUpdate = metadataDate[0].get('date') \n",
    "            dateType = metadataDate[0].get('dateType')\n",
    "        else:\n",
    "            if len(metadataDate) > 1:\n",
    "                for i in metadataDate:\n",
    "                    if i.get('dateType') == \"lastUpdate\":\n",
    "                        lastUpdate = (i.get('date')).split('T')[0]\n",
    "                        dateType = 'last updated'\n",
    "                    else: \n",
    "                        dateType = i.get('dateType')\n",
    "                        lastUpdate = (i.get('date')).split('T')[0]                   \n",
    "\n",
    "        #parse resource Type info\n",
    "        typelist = resourceType[0]\n",
    "        typee = typelist.get('type')\n",
    "        typeename = typelist.get('name')\n",
    "        if typeename != None: \n",
    "            typeename = removeComma(typelist.get('name'))\n",
    "\n",
    "        #parse citation info\n",
    "        title = removeComma(citation.get('title')) #harvested as title to fields\n",
    "        dates = citation.get('date')\n",
    "        responsibleParty = citation.get('responsibleParty')\n",
    "        altTitle = citation.get('alternateTitle')\n",
    "        if altTitle != None:\n",
    "            altTitle = listToString(altTitle)#Harvested as altTitle to fields\n",
    "            altTitle = removeComma(altTitle)\n",
    "        \n",
    "        #Get and format startDate and endDate\n",
    "        try:\n",
    "            startDate = (timePeriod.get('startDateTime','None')).split('T')[0]\n",
    "            end = timePeriod.get('endDateTime', 'None')\n",
    "            if end == None:\n",
    "                endDate = 'onGoing'\n",
    "            else:\n",
    "                endDate = end.split('T')[0]\n",
    "        except:\n",
    "            startDate = 'None'\n",
    "            endDate = 'None'\n",
    "            \n",
    "            \n",
    "        #['EARTH SCIENCE > BIOLOGICAL CLASSIFICATION > ANIMALS/VERTEBRATES > BIRDS > SANDPIPERS', \n",
    "        #'biota', 'intelligenceMilitary', 'environment', 'yellowlegs', 'shorebird', 'migration', 'life cycle']\n",
    "        \n",
    "        #create empty list for keywords\n",
    "        klist =[]\n",
    "        #loop through keyword thesaurus and add keywords to keyword list\n",
    "        try:\n",
    "             #loop through keyword thesaurus and add keywords to keyword list\n",
    "            for g in range(0, len(keyword)):\n",
    "                word = keyword[g]\n",
    "                #print(word.keys())\n",
    "                word1 = word.get('keyword') #list of variable length\n",
    "                #print(word1)\n",
    "                for h in range(0, len(word1)):\n",
    "                    word2 = word1[h]\n",
    "                    word3 = word2.get('keyword')#string\n",
    "                    word4 = word.get('keywordType')\n",
    "                    word5 = word.get('thesaurus')#dict_keys(['date', 'title', 'edition', 'onlineResource', 'identifier'])\n",
    "                    ktype = word5.get('title') \n",
    "                    if ktype == 'Global Change Master Directory (GCMD) Science Keywords':\n",
    "                        parseword = word3.split('>')\n",
    "                        klist.append(parsedword.lower())\n",
    "                    else:\n",
    "                        klist.append(word3)\n",
    "\n",
    "            keywords = listToString(klist)\n",
    "            \n",
    "        except:\n",
    "            keywords = 'None'\n",
    "            \n",
    "        #parse species names from taxonomy; may need to loop if more than one species.\n",
    "        try: # check for taxonomy entry\n",
    "            taxdic = taxonomy[0]\n",
    "            taxClass = taxdic.get('taxonomicClassification')\n",
    "            taxSys = taxdic.get('taxonomicSystem')\n",
    "            taxvoucher = taxdic.get('voucher')\n",
    "            taxClass1 = taxClass[0]\n",
    "            taxClass1.keys()\n",
    "            taxSysID = taxClass1.get('taxonoicSystemID')\n",
    "            taxLevel = taxClass1.get('taxonomicLevel')\n",
    "            taxName = taxClass1.get('taxonomicName')\n",
    "            taxSubClass = taxClass1.get('subClassification')\n",
    "            taxIs = taxClass1.get('isITIS')\n",
    "            taxSub0 = taxSubClass[0]\n",
    "            taxSysID0 = taxSub0.get('taxonoicSystemID')\n",
    "            taxLevel0 = taxSub0.get('taxonomicLevel')\n",
    "            taxName0 = taxSub0.get('taxonomicName')\n",
    "            taxSubClass0 = taxSub0.get('subClassification')\n",
    "            taxIs0 = taxSub0.get('isITIS')\n",
    "            subKingdom =taxSubClass0[0]\n",
    "            infraKingdomL = subKingdom.get('subClassification')\n",
    "            infraKingdom = infraKingdomL[0]\n",
    "            phylumL = infraKingdom.get('subClassification')\n",
    "            phylum = phylumL[0]\n",
    "            subphylumL = phylum.get('subClassification')\n",
    "            subphylum = subphylumL[0]\n",
    "            infraphylumL = subphylum.get('subClassification')\n",
    "            infraphylum = infraphylumL[0]\n",
    "            superclassL = infraphylum.get('subClassification')\n",
    "            superclass = superclassL[0]\n",
    "            classL = superclass.get('subClassification')\n",
    "            classD = classL[0]\n",
    "            orderL = classD.get('subClassification')\n",
    "            order = orderL[0]\n",
    "            familyL = order.get('subClassification')\n",
    "            family = familyL[0]\n",
    "            genusL = family.get('subClassification')\n",
    "            genus = genusL[0]\n",
    "            taxname = genus.get('taxonomicName') #harvested to fields\n",
    "            t =[taxname]\n",
    "            comname = genus.get('commonName') #harvested to fields\n",
    "            t.append(comname)\n",
    "            comname = listToString(comname)\n",
    "            comname = removeComma(comname)\n",
    "        except:  # if no taxomony, then assign 'none'\n",
    "            taxname = \"none\"\n",
    "            comname = \"none\"  \n",
    "    \n",
    "    #Get Associated Resource Info\n",
    "    #if assocated resources are selected from metadata records.... then:\n",
    "        try:\n",
    "            assoclist = []  # create empty list of assocated resource\n",
    "            assocdic = associatedResource[0] #dict_keys(['resourceType', 'resourceCitation', 'associationType', 'initiativeType'])\n",
    "            aresourceType = assocdic.get('resourceType')\n",
    "            aresourceCitation = assocdic.get('resourceCitation')\n",
    "            aassociationType = assocdic.get('associationType')#harvest as associationType ie parentProject\n",
    "            ainitiativeType = assocdic.get('initiativeType') #initiativeType ie project\n",
    "            if aassociationType == 'parentProject' and aresourceType != None:  #this is product metadata record\n",
    "                aresourceType1 = aresourceType[0] #i.e.{'type': 'project', 'name': 'Lesser Yellowlegs Ecology'}\n",
    "                atype = aresourceType1.get('type') #get first entry to remove brackets\n",
    "                aname = aresourceType1.get('name')\n",
    "                addassoc = \"parentProject: \" + aname\n",
    "                assoclist.append(addassoc)\n",
    "            elif aassociationType == 'product': #this is project metadata record\n",
    "                for a in range (0, len(associatedResource)):\n",
    "                    l = associatedResource[a]\n",
    "                    k = l.get('mdRecordId')\n",
    "                    assoclist.append(k)\n",
    "            else:\n",
    "                assoclist = 'no assocated records'\n",
    "        except:\n",
    "            assoclist = 'no assocated records present'\n",
    "        \n",
    "        assoclistString = '; '.join(assoclist)\n",
    "        \n",
    "    \n",
    "    #get resourceDistribution metada\n",
    "        resourceDistribution = metadata.get('resourceDistribution')\n",
    "        distlist = {} #create empty distribution list\n",
    "        try:\n",
    "            for d in range(0, len(resourceDistribution)): #iterate through resourceDistribution info list\n",
    "                distributor = resourceDistribution[d]\n",
    "                dist = dict(distributor)\n",
    "                dist0 = dist.get(\"distributor\")\n",
    "                dist1 = dist0[0] #dictionary keys = 'contact', 'transferOption'\n",
    "                contact = dist1.get('contact')\n",
    "                order = dist1.get('orderProcess')\n",
    "                transopt = dist1.get('transferOption')\n",
    "                distrole =contact.get('role') #harvested as distributor role to distlist\n",
    "                distparty = contact.get('party') #distrbutor contact identifiers\n",
    "                #if len(distparty) > 1:\n",
    "                    #for org in range(0,len(distparty)): \n",
    "                        #distID = distparty[0]\n",
    "                        #distributorID = distID.get('contactId') #harvest distributor ID & compare with contact master list be\n",
    "                #else:\n",
    "                distID = distparty[0]\n",
    "                distributorID = distID.get('contactId') #harvest distributor ID & compare with contact master list be\n",
    "                transopt1 = transopt[0]\n",
    "                transopt2 = transopt1.get('onlineOption')\n",
    "                transopt3 = transopt2[0]\n",
    "                onlineName = transopt3.get('name') #harvested to distlist\n",
    "                onlineUri = transopt3.get('uri') #harvested to distlist\n",
    "                distInfo =[distrole, distributorID, onlineName, onlineUri]\n",
    "                distlist[d] = distInfo\n",
    "                #print (\"distlist = \", distlist)\n",
    "                distInfoString = '; '.join(distInfo)\n",
    "            \n",
    "        except:\n",
    "            print(title, ' has NO distribution metadata')\n",
    "            distInfoString = ' '\n",
    "      \n",
    "    #POINTS of CONTACT\n",
    "        #read Master Contact JSON metadata file into dataframe\n",
    "        contactmetadata = pd.read_json(contact_md)\n",
    "        contactmd1 = dict(contactmetadata)\n",
    "        contactmd2 = contactmd1.get('data')\n",
    "        \n",
    "        POC = collections.defaultdict(list) # create empty dictionary for contacts\n",
    "        POCvalues = []\n",
    "        count = 0\n",
    "\n",
    "        #iterate through master contact metadata\n",
    "        for k in contactmd2:\n",
    "            contactmd3 = contactmd2[count]\n",
    "            contactmd4 = contactmd3.get('attributes')\n",
    "            contactmd5 = contactmd4.get('json')\n",
    "            contactmd6 = json.loads(contactmd5)\n",
    "            contactmd7 = dict(contactmd6)\n",
    "            contactIDmd = contactmd7.get('contactId') #harvest id#\n",
    "            count += 1\n",
    "\n",
    "            for id in distlist:\n",
    "                if distlist[id][1] == contactIDmd:\n",
    "                    contactisOrganizationmd = contactmd7.get('isOrganization')\n",
    "                    contactName = contactmd7.get('name') #havest as distributor name to fields\n",
    "                    contactMemberOf = contactmd7.get('memberOfOrganization')\n",
    "                    contactemail = contactmd7.get('electronicMailAdddress')\n",
    "                    contactType = contactmd7.get('contactType')\n",
    "                    distlist[id][1] = contactName \n",
    "\n",
    "            # iterate through contacts from metadata\n",
    "            for j in pointOfContact:\n",
    "                party = j.get('party')\n",
    "                for p in range(0, len(party)):\n",
    "                    partyContact = party[p]\n",
    "                    partyContactID = partyContact.get('contactId') #id to compare in master contact list\n",
    "                    role = j.get('role')\n",
    "\n",
    "                    #compare master list contact ID with metadata contact ID\n",
    "                    if contactIDmd == partyContactID:\n",
    "                        contactisOrganizationmd = contactmd7.get('isOrganization')\n",
    "                        contactName = contactmd7.get('name')\n",
    "                        contactMemberOf = contactmd7.get('memberOfOrganization')\n",
    "                        contactemail = contactmd7.get('electronicMailAdddress')\n",
    "                        contactType = contactmd7.get('contactType')\n",
    "                        POC[role].append(contactName)\n",
    "                    #else:\n",
    "                        continue\n",
    "\n",
    "        owner = listToString(POC['owner'])\n",
    "        PointOC = listToString(POC['pointOfContact'])\n",
    "        princ = listToString(POC['principalInvestigator'])\n",
    "        custodian = listToString(POC['custodian'])\n",
    "        admin = listToString(POC['administrator'])\n",
    "        originator = listToString(POC['originator'])\n",
    "        contributor = listToString(POC['contributor'])\n",
    "        #distlistString = '; '.join(distInfo) \n",
    "\n",
    "   #Write vales to CSV\n",
    "        #fields = [ID, typee, title, altTitle, typeename, purpose, abstract, shortAbstract, \n",
    "                 # PointOC, owner, princ, custodian, admin, originator, contributor, startDate, endDate, lastUpdate, status, \n",
    "                  #metaIdentifier, metadataStatus, keywords, taxname, comname, distInfoString, assoclistString] #gEid?\n",
    "       \n",
    "        #write files to csv.\n",
    "        #with open (csvname, 'a', newline = '') as csvfile:\n",
    "         #   csvwriter = csv.writer(csvfile)\n",
    "          #  csvwriter.writerow(fields)     \n",
    "   \n",
    "        #Extent to geodataframe\n",
    "        if typee == 'project':  #only gather extents from projects?\n",
    "            extenlist = (extent[0])\n",
    "            try:\n",
    "                extenDisc = extenlist['description'] #harvest as Extent Description?\n",
    "            except:\n",
    "                extenDisc = 'noExtentDiscription'\n",
    "            extenGeo = extenlist['geographicExtent']\n",
    "            extenGeo1 = extenGeo[0]\n",
    "            #extenBox = extenGeo1['boundingBox']\n",
    "            extenGeoElement = extenGeo1['geographicElement'] #type = list\n",
    "\n",
    "            geoInput =[] #empty list for geoinput to geodataframe\n",
    "            poly = gpd.GeoDataFrame(columns = ['id', 'name','geometry', 'type', 'title', 'altTitle', 'typename', \n",
    "                                               'purpose', 'abstract', 'shortAb', 'PointOC', 'trustee', 'PI', \n",
    "                                               'custodian', 'admin', 'origin', 'contrib', 'startDate', 'endDate', 'lastUpdat',\n",
    "                                               'status', 'metaIdent', 'metaStatus', 'keywords', 'taxname', 'comname',\n",
    "                                               'distib', 'assoc']) #gEid?\n",
    "            for ex in range(0, len(extenGeoElement)): #need id, name, descripiton, geometry\n",
    "                gElement = extenGeoElement[ex]  #=dict_keys(['type', 'id', 'geometry', 'properties']) or ['type', 'features']\n",
    "                if gElement.get('type') == \"FeatureCollection\":\n",
    "                    extrastep = gElement.get('features')\n",
    "                    nextstep = extrastep[0] #=dict_keys(['type', 'id', 'geometry', 'properties'])\n",
    "                    gtype = nextstep.get('type') #Feature\n",
    "                    gEid = nextstep.get('id') #harvest as GeoID to fields\n",
    "                    gEgeometry = nextstep.get('geometry') #type=dict_keys(['type', 'coordinates'])\n",
    "                    ggtype = gEgeometry.get('type')\n",
    "                    gcoordinates = gEgeometry.get('coordinates')#list\n",
    "                    gEcoordinates = gcoordinates[0] #list length = 1\n",
    "                    if len(gEcoordinates) == 1:\n",
    "                        gEcoordinates = gEcoordinates[0]\n",
    "                \n",
    "                    #poly_coord = Polygon(gEcoordinates)\n",
    "                    gEproperties = nextstep.get('properties')\n",
    "                    gname = gEproperties.get ('name', 'NotNamed') #harvested to geodataframe\n",
    "                    propertyDesc = gEproperties.get('description', 'NotDescribed')\n",
    "                    print(propertyDesc)\n",
    "                    \n",
    "                    if ggtype == 'Polygon':\n",
    "                        #gcoordinates = gEgeometry.get('coordinates')#list\n",
    "                        #gEcoordinates = gcoordinates[0] #list\n",
    "                        print ('this is polygon')\n",
    "                        poly_coord = Polygon(gEcoordinates)\n",
    "                        geoattributes = {'id':gEid, 'name':gname, 'geometry':poly_coord, 'type':typee, 'title':title, \n",
    "                            'altTitle':altTitle, 'typename':typeename, 'purpose':purpose, 'abstract':abstract, 'shortAb':shortAbstract, \n",
    "                            'PointOC':PointOC, 'trustee':owner, 'PI':princ, 'custodian':custodian, 'admin':admin,\n",
    "                            'origin':originator, 'contrib':contributor, 'startDate':startDate, \n",
    "                            'endDate':endDate, 'lastUpdat':lastUpdate, 'status':status, \n",
    "                            'metaIdent':metaIdentifier, 'metaStatus':metadataStatus, 'keywords':keywords, 'taxname':taxname,\n",
    "                            'comname':comname, 'distrib':distInfoString, 'assoc':assoclistString} #gEid?} #creating dict of geoattrit of geoattribute\n",
    "                        geoInput.append(geoattributes)\n",
    "                    elif ggtype == 'Point':\n",
    "                        #ptcoordinates = gEgeometry.get('coordinates')\n",
    "                        pt_coord = Point(gcoordinates)\n",
    "                        #geoattributes = {'id':gEid, 'name':gname, 'geometry':pt_coord} #creating dict of geoattributes\n",
    "                        continue\n",
    "                    elif ggtype == 'MultiPolygon':\n",
    "                        print ('this is multipolygon')\n",
    "                        mpoly_coord = MultiPolygon(gEcoordinates)\n",
    "                        geoattributes = {'id':gEid, 'name':gname, 'geometry':mpoly_coord, 'type':typee, 'title':title, \n",
    "                            'altTitle':altTitle, 'typename':typeename, 'purpose':purpose, 'abstract':abstract, 'shortAb':shortAbstract, \n",
    "                            'PointOC':PointOC, 'trustee':owner, 'PI':princ, 'custodian':custodian, 'admin':admin,\n",
    "                            'origin':originator, 'contrib':contributor, 'startDate':startDate, \n",
    "                            'endDate':endDate, 'lastUpdat':lastUpdate, 'status':status, \n",
    "                            'metaIdent':metaIdentifier, 'metaStatus':metadataStatus, 'keywords':keywords, 'taxname':taxname,\n",
    "                            'comname':comname, 'distrib':distInfoString, 'assoc':assoclistString} #gEid?} #creating dict of geoatttribut                        \n",
    "                        geoInput.append(geoattributes)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                elif gElement.get('type') == \"Feature\":\n",
    "                    gEtype = gElement.get('type') #dict_keys(['type', 'id', 'geometry', 'properties'])\n",
    "                    gEid = gElement.get('id') #harvest as GeoID to fields\n",
    "                    gEgeometry = gElement.get('geometry') #type=dict_keys(['type', 'coordinates'])\n",
    "                    try:\n",
    "                        gEproperties = gElement.get('properties')\n",
    "                        gname = gEproperties.get ('name', 'NotNamed') #harvested to geodataframe\n",
    "                        #propertyDesc = gEproperties.get('description')\n",
    "                    except: \n",
    "                        gname = 'NotDefined'\n",
    "                    gtype = gEgeometry.get('type') #indicates geometry type: Polygon, Point, line\n",
    "                    if gtype == 'Polygon':\n",
    "                        gcoordinates = gEgeometry.get('coordinates')#list\n",
    "                        gEcoordinates = gcoordinates[0] #list\n",
    "                        poly_coord = Polygon(gEcoordinates)\n",
    "                        geoattributes = {'id':gEid, 'name':gname, 'geometry':poly_coord, 'type':typee, 'title':title, \n",
    "                           'altTitle':altTitle, 'typename':typeename, 'purpose':purpose, 'abstract':abstract, 'shortAb':shortAbstract, \n",
    "                            'PointOC':PointOC, 'trustee':owner, 'PI':princ, 'custodian':custodian, 'admin':admin,\n",
    "                            'origin':originator, 'contrib':contributor, 'startDate':startDate, \n",
    "                            'endDate':endDate, 'lastUpdat':lastUpdate, 'status':status, \n",
    "                            'metaIdent':metaIdentifier, 'metaStatus':metadataStatus, 'keywords':keywords, 'taxname':taxname,\n",
    "                            'comname':comname, 'distrib':distInfoString, 'assoc':assoclistString} #gEid?} #creating dict of geoattributes\n",
    "                        geoInput.append(geoattributes)\n",
    "                    elif gtype == 'Point':\n",
    "                        ptcoordinates = gEgeometry.get('coordinates')\n",
    "                        pt_coord = Point(ptcoordinates)\n",
    "                        #geoattributes = {'id':gEid, 'name':gname, 'geometry':pt_coord} #creating dict of geoattributes\n",
    "                        continue\n",
    "                    elif gtype == 'MultiPolygon':\n",
    "                        gcoordinates = gEgeometry.get('coordinates')#list\n",
    "                        gEcoordinates = gcoordinates[0] #list\n",
    "                        mpoly_coord = MultiPolygon(gEcoordinates)\n",
    "                        geoattributes = {'id':gEid, 'name':gname, 'geometry':mpoly_coord, 'type':typee, 'title':title, \n",
    "                            'altTitle':altTitle, 'typename':typeename, 'purpose':purpose, 'abstract':abstract, 'shortAb':shortAbstract, \n",
    "                            'PointOC':PointOC, 'trustee':owner, 'PI':princ, 'custodian':custodian, 'admin':admin,\n",
    "                            'origin':originator, 'contrib':contributor, 'startDate':startDate, \n",
    "                            'endDate':endDate, 'lastUpdat':lastUpdate, 'status':status, \n",
    "                            'metaIdent':metaIdentifier, 'metaStatus':metadataStatus, 'keywords':keywords, 'taxname':taxname,\n",
    "                            'comname':comname, 'distrib':distInfoString, 'assoc':assoclistString} #gEid?} #creating dict of geoattributes\n",
    "                        geoInput.append(geoattributes)\n",
    "                    else:\n",
    "                        continue    \n",
    "            \n",
    "            poly = gpd.GeoDataFrame(geoInput, geometry = 'geometry', crs = \"EPSG:4326\")  #crs = lat, long designation \n",
    "            polyname = str(workspace + title[0:12] +'.shp') #generate name for shapefile \n",
    "\n",
    "                #if geography is desired, then merge metadata with extent and output shapefile\n",
    "                #if outShape == True:\n",
    "                #Create shapefile from csv with geographic info\n",
    "                #metadf = pd.read_csv(csvname, encoding = 'cp1252') #read completed csv into dataframe\n",
    "\n",
    "                #Subset for projects only\n",
    "                #projectOnly = pd.DataFrame(metadf.loc[metadf['typee'] == 'project'])\n",
    "\n",
    "                #merge geodataframe with metadata dataframe\n",
    "                #dfmerge = pd.merge(poly, projectOnly, how='cross')#,left_on='id',right_on='GeoID')\n",
    "\n",
    "            #write shapefile\n",
    "            poly.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n",
    "            print(gElement, ' ', gtype, ggtype,\" shapefile created.\", \"Number of contacts in master list = \", len(contactmd2))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db6d17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#testing for individual metadata files:\n",
    "\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\PacificFlywayWinterBrantSurvey-mdeditor-20220705-150787.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\EielsonAvianStudy-mdeditor-20220627-170675.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\SeaDuckAtlas-mdeditor-20220629-110616.json'\n",
    "##metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\LesserYellowlegEcology-mdeditor-20220705-210739.json'\n",
    "metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\mbmwa_013_Izembek_Fall_Brant_Survey-init-mdeditor-20221110.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\RedKnotStudy-mdeditor-20220627-160603.json'\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\AKContactsmdeditor-20211228-101265.json'\n",
    "\n",
    "# Pathway to csv file where to write metaata\n",
    "csvname = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\CatalogCSV\\\\\\\\catalogCSVvtest.csv'\n",
    "\n",
    "workspace = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\MBMExtentTest\\\\'\n",
    "\n",
    "mdEditor_read(metadataToRead, contact_md, csvname, workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eabb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e2817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmjv_001_SeaDuck_Key_Habitat_Atlas\\metadata\\mbm7jv_001_seaDuckAtlas_mdeditor-20241115-171152.json\n",
      "Sea Duck Key Habitat Sites Atlas  has NO distribution metadata\n",
      "simplified extent of Sea Duck Key Habitat Extent for North America\n",
      "this is polygon\n",
      "{'type': 'FeatureCollection', 'features': [{'type': 'Feature', 'id': '43685703-bedf-4a3d-a883-f708203a09ae', 'geometry': {'type': 'Polygon', 'coordinates': [[[-168.0523, 70.028336], [-167.700825, 50.741789], [-146.612325, 58.454607], [-131.147425, 46.563122], [-115.33105, 22.589835], [-72.099626, 24.523977], [-50.659651, 50.963814], [-75.262901, 76.108029], [-168.0523, 70.028336]]]}, 'properties': {'name': 'Sea Duck Atlas extent', 'description': 'simplified extent of Sea Duck Key Habitat Extent for North America'}}]}   Feature Polygon  shapefile created. Number of contacts in master list =  759\n",
      "Sea Duck Key Habitat Sites Atlas Document  has NO distribution metadata\n",
      "Sea Duck Key Habitat Sites Atlas data files  has NO distribution metadata\n",
      "mdJSON read successfully\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmjv_002_SDJV158_Isotopes\\metadata\\mbm7jv_002_mdeditor-20250225-110237.json\n",
      "Identifying the diets and breeding areas of harvested juvenile sea ducks: a continued stable isotope investigation  has NO distribution metadata\n",
      "NotDescribed\n",
      "this is polygon\n",
      "{'type': 'FeatureCollection', 'features': [{'type': 'Feature', 'id': 'b29dcdb6-5324-46ab-93b5-102c16061dcc', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.163507, 19.808049], [-107.053656, 22.105994], [-120.754166, 35.317363], [-125.321002, 45.951148], [-132.6982, 54.572061], [-144.99353, 59.800634], [-164.490409, 54.775345], [-168.705951, 63.704723], [-168.530303, 70.786911], [-131.819962, 70.959698], [-124.79406, 76.184997], [-105.121533, 79.496653], [-74.207561, 73.124946], [-59.453166, 65.946472], [-55.764567, 52.05249], [-52.626172, 47.754095], [-80.90543, 31.353632], [-79.851545, 24.527129], [-96.163507, 19.808049]]]}, 'properties': {'name': 'North America'}}]}   Feature Polygon  shapefile created. Number of contacts in master list =  759\n",
      "A continued stable isotope investigation identifying the diets and breeding areas of harvested juvenile sea ducks data table  has NO distribution metadata\n",
      "A continued stable isotope investigation identifying the diets and breeding areas of harvested juvenile sea ducks Final Report  has NO distribution metadata\n",
      "SDJV 158: PhD Dissertation  has NO distribution metadata\n",
      "SDJV 158: Peer-reviewed Publication  has NO distribution metadata\n",
      "mdJSON read successfully\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmjv_004_SDJV176_ScoterIPM\\metadata\\mbm7jv_004-mdeditor-20250225-070260.json\n",
      "An integrated model of scoter populations in eastern North America with a focus on estimating survival  has NO distribution metadata\n",
      "NotDescribed\n",
      "this is polygon\n",
      "{'type': 'FeatureCollection', 'features': [{'type': 'Feature', 'id': '413915d2-98e5-4c8c-a57a-9bf8b09abf53', 'geometry': {'type': 'Polygon', 'coordinates': [[[-77.6953125, 62.91523303947614], [-61.17187499999999, 59.712097173322924], [-53.0859375, 50.736455137010665], [-68.5546875, 40.97989806962013], [-78.75, 30.751277776257812], [-75.5859375, 24.206889622398023], [-60.1171875, 10.14193168613103], [-42.890625, 0.7031073524364909], [-33.046875, -5.615985819155327], [-39.0234375, -19.973348786110602], [-60.1171875, -40.97989806962013], [-67.1484375, -45.08903556483102], [-63.6328125, -9.102096738726443], [-71.015625, 9.102096738726456], [-83.671875, 21.94304553343818], [-86.484375, 30.14512718337613], [-77.6953125, 46.31658418182218], [-75.9375, 49.61070993807422], [-77.6953125, 62.91523303947614]]]}, 'properties': {'name': 'Atlantic Flyway'}}]}   Feature Polygon  shapefile created. Number of contacts in master list =  759\n",
      "SDJV176 Final Report  has NO distribution metadata\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m jfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root,name)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(jfile)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mmdEditor_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact_md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsvname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m MBMmetadataNo \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmdJSON read successfully\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mmdEditor_read\u001b[1;34m(metadataToRead, contact_md, csvname, workspace, recordtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m citation \u001b[38;5;241m=\u001b[39m resourceInfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     77\u001b[0m pointOfContact \u001b[38;5;241m=\u001b[39m resourceInfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpointOfContact\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m abstract \u001b[38;5;241m=\u001b[39m \u001b[43mremoveComma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresourceInfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#harvested to fields\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resourceInfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortAbstract\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     shortAbstract \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mmdEditor_read.<locals>.removeComma\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremoveComma\u001b[39m(string): \u001b[38;5;66;03m#define remove comma function\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "#Search Migratory Birds Management RDR folder for mdeditor files to extract metadata\n",
    "#Count number of preserved mdEditor records\n",
    "import os\n",
    "RDR = '\\\\\\\\ifw7ro-file.fws.doi.net\\\\datamgt\\\\mbm'\n",
    "MBMmetadataNo = 0\n",
    "program = \"Migratory Bird Manangement\"\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\AK_contacts_profiles\\\\AK-contacts-mdeditor-20250122-150150.json'\n",
    "\n",
    "# Pathway to csv file where to write metadata\n",
    "csvname = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\catalogCSVPhase2v2.csv'\n",
    "\n",
    "workspace = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\MBMExtentTest\\\\'\n",
    "\n",
    "#loop through RDR folder structure and find mdeditor json files that is NOT in incoming folder\n",
    "for root, dirs, files in os.walk(RDR,topdown=True):\n",
    "    #print (\"root=\", root, \"  dirs=\", dirs, \"  file=\", files)\n",
    "    for name in files:\n",
    "        if 'incoming' not in root and 'mdeditor' in name and '-init-' not in name and name.endswith('.json'):\n",
    "            jfile = os.path.join(root,name)\n",
    "            print(jfile)\n",
    "            mdEditor_read(jfile, contact_md, csvname, workspace)\n",
    "            MBMmetadataNo += 1\n",
    "            print ('mdJSON read successfully')\n",
    "print (\"Number of MBM completed mdeditor records in RDR = \",MBMmetadataNo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search FES RDR folder for mdeditor files to extract metadata\n",
    "#Count number of preserved mdEditor records\n",
    "import os\n",
    "RDR = '\\\\\\\\ifw7ro-file.fws.doi.net\\\\datamgt\\\\fes'\n",
    "MBMmetadataNo = 0\n",
    "program = \"Fisheries & Ecological Services\"\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\AKContactsmdeditor-20211228-101265.json'\n",
    "\n",
    "# Pathway to csv file where to write metaata\n",
    "csvname = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\CatalogCSV\\\\\\\\catalogCSVv3.csv'\n",
    "\n",
    "#loop through RDR folder structure and find mdeditor json files that is NOT in incoming folder\n",
    "for root, dirs, files in os.walk(RDR,topdown=True):\n",
    "    #print (\"root=\", root, \"  dirs=\", dirs, \"  file=\", files)\n",
    "    for name in files:\n",
    "        if 'incoming' not in root and 'mdeditor' in name and name.endswith('.json'):\n",
    "            jfile = os.path.join(root,name)\n",
    "            mdEditor_read(jfile, contact_md, csvname)\n",
    "            MBMmetadataNo += 1\n",
    "            print (jfile)\n",
    "print (\"Number of FES completed mdeditor records in RDR = \",MBMmetadataNo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0aca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
