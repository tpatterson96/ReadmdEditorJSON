{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to read mdEditor files, and write out as shapefile\n",
    "#and shapefile with geography, if applicable\n",
    "#by Tamatha A. Patterson; verson 5; December 2022\n",
    "#distribution section updated.\n",
    "#extent merged with metadata and written to shapefile. #FeatureCollection type handled.\n",
    "#associations section updated.\n",
    "\n",
    "def mdEditor_read(metadataToRead, contact_md, csvname, workspace, recordtype = 'all'):\n",
    "    #recordtype used to write chosen resource types  \\\"all\\\", \\\"project\\\", etc.in future versions\n",
    "    #extent/geography read when is True, skipped when False.\n",
    "    import os\n",
    "    import json\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import collections\n",
    "    import fiona\n",
    "    from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
    "    from datetime import date\n",
    "    \n",
    "    def removeComma(string): #define remove comma function\n",
    "        return string.replace(\",\",\"; \")\n",
    "    \n",
    "    def listToString(s): #define converting list to a string format\n",
    "        str1 = \"; \"\n",
    "        return (str1.join(s))\n",
    "    \n",
    "    def def_value():\n",
    "        return \"none\"\n",
    "    \n",
    "    os.chdir(workspace) # assign working directory\n",
    "    today = date.today()\n",
    "    \n",
    "    df = pd.read_json(metadataToRead)#read JSON metadata files into dataframe\n",
    "    values = df.get('data') #assigns metadata to values\n",
    "    for e in range(0, len(values)): #json file may have multiple metadata records\n",
    "        element = values[e] #assign list value e containing the metadata #keys= id, attributes, type\n",
    "        ID = element.get('id') #get metadata id\n",
    "        attribute = element.get('attributes') # keys= profile, json, data-updated\n",
    "        typpe = element.get('type') #get metadata type\n",
    "        if typpe != 'records':  \n",
    "            # skip if metadata is not a record and is a data dictionary, setting, schemas, custom-profiles...\n",
    "            continue  #go to next record\n",
    "\n",
    "        #parse 'attribute' to create profile, json, and date-updated\\n\",\n",
    "        dateUpdate = attribute.get('date-updated') #create dateUpdate value--where does this date come from???\n",
    "        jsondata = attribute.get('json') #create'json' data value\n",
    "        profile = attribute.get('profile') #create 'profile' value\n",
    "        \n",
    "        #convert string to dictionary\n",
    "        jsondatadict = json.loads(jsondata)  #3 keys = schema, metadata, mdDictionary\n",
    "        schema = jsondatadict.get('schema')\n",
    "        metadata = jsondatadict.get('metadata') #4 keys = metadataInfo, resourceInfo, associatedResource, resourceDistribution\\n\",\n",
    "        mdDictionary = jsondatadict.get('mdDictionary')\n",
    "        #get metadata key entries\n",
    "        metadataInfo = metadata.get('metadataInfo') #6 Keys = metadataIdentifier, metadataContact, defaultMetadataLocale, metadataDate, parentMetadata, metadataStatus\n",
    "        resourceInfo = metadata.get('resourceInfo') #12 keys = resourceType, citation, pointOfContact, abstract, shortAbstract, status, defaultResourceLocale, extent, keyword, purpose, taxonomy, timePeriod\n",
    "        associatedResource = metadata.get('associatedResource') #is list\n",
    "        resourceDistribution = metadata.get('resourceDistribution') #keys = n\n",
    "\n",
    "        #parse metadataInfo dictionary 6 keys\n",
    "        metadataIdentifier = metadataInfo.get('metadataIdentifier') #Harvest as ID to fields\n",
    "        metadataContact = metadataInfo.get('metadataContact')\n",
    "        defaultMetadataLocale = metadataInfo.get('defaultMetadataLocale')\n",
    "        metadataDate = metadataInfo.get('metadataDate')\n",
    "        parentMetadata = metadataInfo.get('parentMetadata')\n",
    "        metadataStatus = metadataInfo.get('metadataStatus') #Harvest as status to fields\n",
    "\n",
    "        #get metadata uuid identifier; autocreated in mdEditor\\n\",\n",
    "        if metadataIdentifier['namespace'] == 'urn:uuid':\n",
    "                metaIdentifier = metadataIdentifier.get('identifier')\n",
    "\n",
    "        #parse resourceInfo 12 keys: 'resourceType', 'citation', 'pointOfContact', 'abstract', 'shortAbstract', 'status', \\n\",\n",
    "        #...'defaultResourceLocale', 'extent', 'keyword', 'purpose', 'taxonomy', 'timePeriod'\\n\",\n",
    "        resourceType = resourceInfo.get('resourceType')\n",
    "        citation = resourceInfo.get('citation')\n",
    "        pointOfContact = resourceInfo.get('pointOfContact')\n",
    "        abstract = removeComma(resourceInfo.get('abstract')) #harvested to fields\n",
    "        if resourceInfo.get('shortAbstract') == None:\n",
    "            shortAbstract = \" \"\n",
    "        else:\n",
    "            shortAbstract = removeComma(resourceInfo.get('shortAbstract'))#harvested as shortAbstract to fields\n",
    "        statusList = resourceInfo.get('status')\n",
    "        status = statusList[0]#harvest as status to fields\n",
    "        defaultResourceLocale = resourceInfo.get('defaultResourceLocale')\n",
    "        extent = resourceInfo.get('extent')\n",
    "        keyword = resourceInfo.get('keyword')\n",
    "        if resourceInfo.get('purpose') == None:\n",
    "            purpose = \" \"\n",
    "        else:\n",
    "            purpose = removeComma(resourceInfo.get('purpose')) #harvested to fields\n",
    "        taxonomy = resourceInfo.get('taxonomy')\n",
    "        timePeriod = resourceInfo.get('timePeriod')\n",
    "\n",
    "        #find last update date from metadataDate\n",
    "        #Consider comparing this to last run date and only reading metadata updated after????\n",
    "        if len(metadataDate) == 1:\n",
    "            lastUpdate = metadataDate[0].get('date') \n",
    "            dateType = metadataDate[0].get('dateType')\n",
    "        else:\n",
    "            if len(metadataDate) > 1:\n",
    "                for i in metadataDate:\n",
    "                    if i.get('dateType') == \"lastUpdate\":\n",
    "                        lastUpdate = (i.get('date')).split('T')[0]\n",
    "                        dateType = 'last updated'\n",
    "                    else: \n",
    "                        dateType = i.get('dateType')\n",
    "                        lastUpdate = (i.get('date')).split('T')[0]                   \n",
    "\n",
    "        #parse resource Type info\n",
    "        typelist = resourceType[0]\n",
    "        typee = typelist.get('type')\n",
    "        typeename = typelist.get('name')\n",
    "        if typeename != None: \n",
    "            typeename = removeComma(typelist.get('name'))\n",
    "\n",
    "        #parse citation info\n",
    "        title = removeComma(citation.get('title')) #harvested as title to fields\n",
    "        dates = citation.get('date')\n",
    "        responsibleParty = citation.get('responsibleParty')\n",
    "        altTitle = citation.get('alternateTitle')\n",
    "        if altTitle != None:\n",
    "            altTitle = listToString(altTitle)#Harvested as altTitle to fields\n",
    "            altTitle = removeComma(altTitle)\n",
    "        \n",
    "        #Get and format startDate and endDate\n",
    "        try:\n",
    "            startDate = (timePeriod.get('startDateTime','None')).split('T')[0]\n",
    "            end = timePeriod.get('endDateTime', 'None')\n",
    "            if end == None:\n",
    "                endDate = 'onGoing'\n",
    "            else:\n",
    "                endDate = end.split('T')[0]\n",
    "        except:\n",
    "            startDate = 'None'\n",
    "            endDate = 'None'\n",
    "            \n",
    "            \n",
    "        #['EARTH SCIENCE > BIOLOGICAL CLASSIFICATION > ANIMALS/VERTEBRATES > BIRDS > SANDPIPERS', \n",
    "        #'biota', 'intelligenceMilitary', 'environment', 'yellowlegs', 'shorebird', 'migration', 'life cycle']\n",
    "        \n",
    "        #create empty list for keywords\n",
    "        klist =[]\n",
    "        #loop through keyword thesaurus and add keywords to keyword list\n",
    "        try:\n",
    "             #loop through keyword thesaurus and add keywords to keyword list\n",
    "            for g in range(0, len(keyword)):\n",
    "                word = keyword[g]\n",
    "                #print(word.keys())\n",
    "                word1 = word.get('keyword') #list of variable length\n",
    "                #print(word1)\n",
    "                for h in range(0, len(word1)):\n",
    "                    word2 = word1[h]\n",
    "                    word3 = word2.get('keyword')#string\n",
    "                    word4 = word.get('keywordType')\n",
    "                    word5 = word.get('thesaurus')#dict_keys(['date', 'title', 'edition', 'onlineResource', 'identifier'])\n",
    "                    ktype = word5.get('title') \n",
    "                    if ktype == 'Global Change Master Directory (GCMD) Science Keywords':\n",
    "                        parseword = word3.split('>')\n",
    "                        klist.append(parsedword.lower())\n",
    "                    else:\n",
    "                        klist.append(word3)\n",
    "\n",
    "            keywords = listToString(klist)\n",
    "            \n",
    "        except:\n",
    "            keywords = 'None'\n",
    "            \n",
    "        #parse species names from taxonomy; may need to loop if more than one species.\n",
    "        try: # check for taxonomy entry\n",
    "            taxdic = taxonomy[0]\n",
    "            taxClass = taxdic.get('taxonomicClassification')\n",
    "            taxSys = taxdic.get('taxonomicSystem')\n",
    "            taxvoucher = taxdic.get('voucher')\n",
    "            taxClass1 = taxClass[0]\n",
    "            taxClass1.keys()\n",
    "            taxSysID = taxClass1.get('taxonoicSystemID')\n",
    "            taxLevel = taxClass1.get('taxonomicLevel')\n",
    "            taxName = taxClass1.get('taxonomicName')\n",
    "            taxSubClass = taxClass1.get('subClassification')\n",
    "            taxIs = taxClass1.get('isITIS')\n",
    "            taxSub0 = taxSubClass[0]\n",
    "            taxSysID0 = taxSub0.get('taxonoicSystemID')\n",
    "            taxLevel0 = taxSub0.get('taxonomicLevel')\n",
    "            taxName0 = taxSub0.get('taxonomicName')\n",
    "            taxSubClass0 = taxSub0.get('subClassification')\n",
    "            taxIs0 = taxSub0.get('isITIS')\n",
    "            subKingdom =taxSubClass0[0]\n",
    "            infraKingdomL = subKingdom.get('subClassification')\n",
    "            infraKingdom = infraKingdomL[0]\n",
    "            phylumL = infraKingdom.get('subClassification')\n",
    "            phylum = phylumL[0]\n",
    "            subphylumL = phylum.get('subClassification')\n",
    "            subphylum = subphylumL[0]\n",
    "            infraphylumL = subphylum.get('subClassification')\n",
    "            infraphylum = infraphylumL[0]\n",
    "            superclassL = infraphylum.get('subClassification')\n",
    "            superclass = superclassL[0]\n",
    "            classL = superclass.get('subClassification')\n",
    "            classD = classL[0]\n",
    "            orderL = classD.get('subClassification')\n",
    "            order = orderL[0]\n",
    "            familyL = order.get('subClassification')\n",
    "            family = familyL[0]\n",
    "            genusL = family.get('subClassification')\n",
    "            genus = genusL[0]\n",
    "            taxname = genus.get('taxonomicName') #harvested to fields\n",
    "            t =[taxname]\n",
    "            comname = genus.get('commonName') #harvested to fields\n",
    "            t.append(comname)\n",
    "            comname = listToString(comname)\n",
    "            comname = removeComma(comname)\n",
    "        except:  # if no taxomony, then assign 'none'\n",
    "            taxname = \"none\"\n",
    "            comname = \"none\"  \n",
    "    \n",
    "    #Get Associated Resource Info\n",
    "    #if assocated resources are selected from metadata records.... then:\n",
    "        try:\n",
    "            assoclist = []  # create empty list of assocated resource\n",
    "            assocdic = associatedResource[0] #dict_keys(['resourceType', 'resourceCitation', 'associationType', 'initiativeType'])\n",
    "            aresourceType = assocdic.get('resourceType')\n",
    "            aresourceCitation = assocdic.get('resourceCitation')\n",
    "            aassociationType = assocdic.get('associationType')#harvest as associationType ie parentProject\n",
    "            ainitiativeType = assocdic.get('initiativeType') #initiativeType ie project\n",
    "            if aassociationType == 'parentProject' and aresourceType != None:  #this is product metadata record\n",
    "                aresourceType1 = aresourceType[0] #i.e.{'type': 'project', 'name': 'Lesser Yellowlegs Ecology'}\n",
    "                atype = aresourceType1.get('type') #get first entry to remove brackets\n",
    "                aname = aresourceType1.get('name')\n",
    "                addassoc = \"parentProject: \" + aname\n",
    "                assoclist.append(addassoc)\n",
    "            elif aassociationType == 'product': #this is project metadata record\n",
    "                for a in range (0, len(associatedResource)):\n",
    "                    l = associatedResource[a]\n",
    "                    k = l.get('mdRecordId')\n",
    "                    assoclist.append(k)\n",
    "            else:\n",
    "                assoclist = 'no assocated records'\n",
    "        except:\n",
    "            assoclist = 'no assocated records present'\n",
    "        \n",
    "        assoclistString = '; '.join(assoclist)\n",
    "        \n",
    "    \n",
    "    #get resourceDistribution metada\n",
    "        resourceDistribution = metadata.get('resourceDistribution')\n",
    "        distlist = {} #create empty distribution list\n",
    "        try:\n",
    "            for d in range(0, len(resourceDistribution)): #iterate through resourceDistribution info list\n",
    "                distributor = resourceDistribution[d]\n",
    "                dist = dict(distributor)\n",
    "                dist0 = dist.get(\"distributor\")\n",
    "                dist1 = dist0[0] #dictionary keys = 'contact', 'transferOption'\n",
    "                contact = dist1.get('contact')\n",
    "                order = dist1.get('orderProcess')\n",
    "                transopt = dist1.get('transferOption')\n",
    "                distrole =contact.get('role') #harvested as distributor role to distlist\n",
    "                distparty = contact.get('party') #distrbutor contact identifiers\n",
    "                #if len(distparty) > 1:\n",
    "                    #for org in range(0,len(distparty)): \n",
    "                        #distID = distparty[0]\n",
    "                        #distributorID = distID.get('contactId') #harvest distributor ID & compare with contact master list be\n",
    "                #else:\n",
    "                distID = distparty[0]\n",
    "                distributorID = distID.get('contactId') #harvest distributor ID & compare with contact master list be\n",
    "                transopt1 = transopt[0]\n",
    "                transopt2 = transopt1.get('onlineOption')\n",
    "                transopt3 = transopt2[0]\n",
    "                onlineName = transopt3.get('name') #harvested to distlist\n",
    "                onlineUri = transopt3.get('uri') #harvested to distlist\n",
    "                distInfo =[distrole, distributorID, onlineName, onlineUri]\n",
    "                distlist[d] = distInfo\n",
    "                #print (\"distlist = \", distlist)\n",
    "                distInfoString = '; '.join(distInfo)\n",
    "            \n",
    "        except:\n",
    "            print(title, ' has NO distribution metadata')\n",
    "            distInfoString = ' '\n",
    "      \n",
    "    #POINTS of CONTACT\n",
    "        #read Master Contact JSON metadata file into dataframe\n",
    "        contactmetadata = pd.read_json(contact_md)\n",
    "        contactmd1 = dict(contactmetadata)\n",
    "        contactmd2 = contactmd1.get('data')\n",
    "        \n",
    "        POC = collections.defaultdict(list) # create empty dictionary for contacts\n",
    "        POCvalues = []\n",
    "        count = 0\n",
    "\n",
    "        #iterate through master contact metadata\n",
    "        for k in contactmd2:\n",
    "            contactmd3 = contactmd2[count]\n",
    "            contactmd4 = contactmd3.get('attributes')\n",
    "            contactmd5 = contactmd4.get('json')\n",
    "            contactmd6 = json.loads(contactmd5)\n",
    "            contactmd7 = dict(contactmd6)\n",
    "            contactIDmd = contactmd7.get('contactId') #harvest id#\n",
    "            count += 1\n",
    "\n",
    "            for id in distlist:\n",
    "                if distlist[id][1] == contactIDmd:\n",
    "                    contactisOrganizationmd = contactmd7.get('isOrganization')\n",
    "                    contactName = contactmd7.get('name') #havest as distributor name to fields\n",
    "                    contactMemberOf = contactmd7.get('memberOfOrganization')\n",
    "                    contactemail = contactmd7.get('electronicMailAdddress')\n",
    "                    contactType = contactmd7.get('contactType')\n",
    "                    distlist[id][1] = contactName \n",
    "\n",
    "            # iterate through contacts from metadata\n",
    "            for j in pointOfContact:\n",
    "                party = j.get('party')\n",
    "                for p in range(0, len(party)):\n",
    "                    partyContact = party[p]\n",
    "                    partyContactID = partyContact.get('contactId') #id to compare in master contact list\n",
    "                    role = j.get('role')\n",
    "\n",
    "                    #compare master list contact ID with metadata contact ID\n",
    "                    if contactIDmd == partyContactID:\n",
    "                        contactisOrganizationmd = contactmd7.get('isOrganization')\n",
    "                        contactName = contactmd7.get('name')\n",
    "                        contactMemberOf = contactmd7.get('memberOfOrganization')\n",
    "                        contactemail = contactmd7.get('electronicMailAdddress')\n",
    "                        contactType = contactmd7.get('contactType')\n",
    "                        POC[role].append(contactName)\n",
    "                    #else:\n",
    "                        continue\n",
    "\n",
    "        owner = listToString(POC['owner'])\n",
    "        PointOC = listToString(POC['pointOfContact'])\n",
    "        princ = listToString(POC['principalInvestigator'])\n",
    "        custodian = listToString(POC['custodian'])\n",
    "        admin = listToString(POC['administrator'])\n",
    "        originator = listToString(POC['originator'])\n",
    "        contributor = listToString(POC['contributor'])\n",
    "        #distlistString = '; '.join(distInfo) \n",
    "\n",
    "   #Write vales to CSV\n",
    "        #fields = [ID, typee, title, altTitle, typeename, purpose, abstract, shortAbstract, \n",
    "                 # PointOC, owner, princ, custodian, admin, originator, contributor, startDate, endDate, lastUpdate, status, \n",
    "                  #metaIdentifier, metadataStatus, keywords, taxname, comname, distInfoString, assoclistString] #gEid?\n",
    "       \n",
    "        #write files to csv.\n",
    "        #with open (csvname, 'a', newline = '') as csvfile:\n",
    "         #   csvwriter = csv.writer(csvfile)\n",
    "          #  csvwriter.writerow(fields)     \n",
    "   \n",
    "        #Extent to geodataframe\n",
    "        if typee == 'project':  #only gather extents from projects?\n",
    "            extenlist = (extent[0])\n",
    "            try:\n",
    "                extenDisc = extenlist['description'] #harvest as Extent Description?\n",
    "            except:\n",
    "                extenDisc = 'noExtentDiscription'\n",
    "            extenGeo = extenlist['geographicExtent']\n",
    "            extenGeo1 = extenGeo[0]\n",
    "            #extenBox = extenGeo1['boundingBox']\n",
    "            extenGeoElement = extenGeo1['geographicElement'] #type = list\n",
    "\n",
    "            geoInput =[] #empty list for geoinput to geodataframe\n",
    "            poly = gpd.GeoDataFrame(columns = ['id', 'name','geometry', 'type', 'title', 'altTitle', 'typename', \n",
    "                                               'purpose', 'abstract', 'shortAb', 'PointOC', 'trustee', 'PI', \n",
    "                                               'custodian', 'admin', 'origin', 'contrib', 'startDate', 'endDate', 'lastUpdat',\n",
    "                                               'status', 'metaIdent', 'metaStatus', 'keywords', 'taxname', 'comname',\n",
    "                                               'distib', 'assoc']) #gEid?\n",
    "            for ex in range(0, len(extenGeoElement)): #need id, name, descripiton, geometry\n",
    "                gElement = extenGeoElement[ex]  #=dict_keys(['type', 'id', 'geometry', 'properties']) or ['type', 'features']\n",
    "                if gElement.get('type') == \"FeatureCollection\":\n",
    "                    extrastep = gElement.get('features')\n",
    "                    nextstep = extrastep[0] #=dict_keys(['type', 'id', 'geometry', 'properties'])\n",
    "                    gtype = nextstep.get('type') #Feature\n",
    "                    gEid = nextstep.get('id') #harvest as GeoID to fields\n",
    "                    gEgeometry = nextstep.get('geometry') #type=dict_keys(['type', 'coordinates'])\n",
    "                    ggtype = gEgeometry.get('type')\n",
    "                    gcoordinates = gEgeometry.get('coordinates')#list\n",
    "                    gEcoordinates = gcoordinates[0] #list length = 1\n",
    "                    if len(gEcoordinates) == 1:\n",
    "                        gEcoordinates = gEcoordinates[0]\n",
    "                \n",
    "                    #poly_coord = Polygon(gEcoordinates)\n",
    "                    gEproperties = nextstep.get('properties')\n",
    "                    gname = gEproperties.get ('name', 'NotNamed') #harvested to geodataframe\n",
    "                    propertyDesc = gEproperties.get('description', 'NotDescribed')\n",
    "                    print(propertyDesc)\n",
    "                    \n",
    "                    if ggtype == 'Polygon':\n",
    "                        #gcoordinates = gEgeometry.get('coordinates')#list\n",
    "                        #gEcoordinates = gcoordinates[0] #list\n",
    "                        print ('this is polygon')\n",
    "                        poly_coord = Polygon(gEcoordinates)\n",
    "                        geoattributes = {'id':gEid, 'name':gname, 'geometry':poly_coord, 'type':typee, 'title':title, \n",
    "                            'altTitle':altTitle, 'typename':typeename, 'purpose':purpose, 'abstract':abstract, 'shortAb':shortAbstract, \n",
    "                            'PointOC':PointOC, 'trustee':owner, 'PI':princ, 'custodian':custodian, 'admin':admin,\n",
    "                            'origin':originator, 'contrib':contributor, 'startDate':startDate, \n",
    "                            'endDate':endDate, 'lastUpdat':lastUpdate, 'status':status, \n",
    "                            'metaIdent':metaIdentifier, 'metaStatus':metadataStatus, 'keywords':keywords, 'taxname':taxname,\n",
    "                            'comname':comname, 'distrib':distInfoString, 'assoc':assoclistString} #gEid?} #creating dict of geoattrit of geoattribute\n",
    "                        geoInput.append(geoattributes)\n",
    "                    elif ggtype == 'Point':\n",
    "                        #ptcoordinates = gEgeometry.get('coordinates')\n",
    "                        pt_coord = Point(gcoordinates)\n",
    "                        #geoattributes = {'id':gEid, 'name':gname, 'geometry':pt_coord} #creating dict of geoattributes\n",
    "                        continue\n",
    "                    elif ggtype == 'MultiPolygon':\n",
    "                        print ('this is multipolygon')\n",
    "                        mpoly_coord = MultiPolygon(gEcoordinates)\n",
    "                        geoattributes = {'id':gEid, 'name':gname, 'geometry':mpoly_coord, 'type':typee, 'title':title, \n",
    "                            'altTitle':altTitle, 'typename':typeename, 'purpose':purpose, 'abstract':abstract, 'shortAb':shortAbstract, \n",
    "                            'PointOC':PointOC, 'trustee':owner, 'PI':princ, 'custodian':custodian, 'admin':admin,\n",
    "                            'origin':originator, 'contrib':contributor, 'startDate':startDate, \n",
    "                            'endDate':endDate, 'lastUpdat':lastUpdate, 'status':status, \n",
    "                            'metaIdent':metaIdentifier, 'metaStatus':metadataStatus, 'keywords':keywords, 'taxname':taxname,\n",
    "                            'comname':comname, 'distrib':distInfoString, 'assoc':assoclistString} #gEid?} #creating dict of geoatttribut                        \n",
    "                        geoInput.append(geoattributes)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                elif gElement.get('type') == \"Feature\":\n",
    "                    gEtype = gElement.get('type') #dict_keys(['type', 'id', 'geometry', 'properties'])\n",
    "                    gEid = gElement.get('id') #harvest as GeoID to fields\n",
    "                    gEgeometry = gElement.get('geometry') #type=dict_keys(['type', 'coordinates'])\n",
    "                    try:\n",
    "                        gEproperties = gElement.get('properties')\n",
    "                        gname = gEproperties.get ('name', 'NotNamed') #harvested to geodataframe\n",
    "                        #propertyDesc = gEproperties.get('description')\n",
    "                    except: \n",
    "                        gname = 'NotDefined'\n",
    "                    gtype = gEgeometry.get('type') #indicates geometry type: Polygon, Point, line\n",
    "                    if gtype == 'Polygon':\n",
    "                        gcoordinates = gEgeometry.get('coordinates')#list\n",
    "                        gEcoordinates = gcoordinates[0] #list\n",
    "                        poly_coord = Polygon(gEcoordinates)\n",
    "                        geoattributes = {'id':gEid, 'name':gname, 'geometry':poly_coord, 'type':typee, 'title':title, \n",
    "                           'altTitle':altTitle, 'typename':typeename, 'purpose':purpose, 'abstract':abstract, 'shortAb':shortAbstract, \n",
    "                            'PointOC':PointOC, 'trustee':owner, 'PI':princ, 'custodian':custodian, 'admin':admin,\n",
    "                            'origin':originator, 'contrib':contributor, 'startDate':startDate, \n",
    "                            'endDate':endDate, 'lastUpdat':lastUpdate, 'status':status, \n",
    "                            'metaIdent':metaIdentifier, 'metaStatus':metadataStatus, 'keywords':keywords, 'taxname':taxname,\n",
    "                            'comname':comname, 'distrib':distInfoString, 'assoc':assoclistString} #gEid?} #creating dict of geoattributes\n",
    "                        geoInput.append(geoattributes)\n",
    "                    elif gtype == 'Point':\n",
    "                        ptcoordinates = gEgeometry.get('coordinates')\n",
    "                        pt_coord = Point(ptcoordinates)\n",
    "                        #geoattributes = {'id':gEid, 'name':gname, 'geometry':pt_coord} #creating dict of geoattributes\n",
    "                        continue\n",
    "                    elif gtype == 'MultiPolygon':\n",
    "                        gcoordinates = gEgeometry.get('coordinates')#list\n",
    "                        gEcoordinates = gcoordinates[0] #list\n",
    "                        mpoly_coord = MultiPolygon(gEcoordinates)\n",
    "                        geoattributes = {'id':gEid, 'name':gname, 'geometry':mpoly_coord, 'type':typee, 'title':title, \n",
    "                            'altTitle':altTitle, 'typename':typeename, 'purpose':purpose, 'abstract':abstract, 'shortAb':shortAbstract, \n",
    "                            'PointOC':PointOC, 'trustee':owner, 'PI':princ, 'custodian':custodian, 'admin':admin,\n",
    "                            'origin':originator, 'contrib':contributor, 'startDate':startDate, \n",
    "                            'endDate':endDate, 'lastUpdat':lastUpdate, 'status':status, \n",
    "                            'metaIdent':metaIdentifier, 'metaStatus':metadataStatus, 'keywords':keywords, 'taxname':taxname,\n",
    "                            'comname':comname, 'distrib':distInfoString, 'assoc':assoclistString} #gEid?} #creating dict of geoattributes\n",
    "                        geoInput.append(geoattributes)\n",
    "                    else:\n",
    "                        continue    \n",
    "            \n",
    "            poly = gpd.GeoDataFrame(geoInput, geometry = 'geometry', crs = \"EPSG:4326\")  #crs = lat, long designation \n",
    "            polyname = str(workspace + title[0:12] +'.shp') #generate name for shapefile \n",
    "\n",
    "                #if geography is desired, then merge metadata with extent and output shapefile\n",
    "                #if outShape == True:\n",
    "                #Create shapefile from csv with geographic info\n",
    "                #metadf = pd.read_csv(csvname, encoding = 'cp1252') #read completed csv into dataframe\n",
    "\n",
    "                #Subset for projects only\n",
    "                #projectOnly = pd.DataFrame(metadf.loc[metadf['typee'] == 'project'])\n",
    "\n",
    "                #merge geodataframe with metadata dataframe\n",
    "                #dfmerge = pd.merge(poly, projectOnly, how='cross')#,left_on='id',right_on='GeoID')\n",
    "\n",
    "            #write shapefile\n",
    "            poly.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n",
    "            print(gElement, ' ', gtype, ggtype,\" shapefile created.\", \"Number of contacts in master list = \", len(contactmd2))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59db6d17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alasaka Izembek Fall Brant Aerial Survey 1976-Present  has NO distribution metadata\n",
      "Izembek National Wildlife Refuge, simplified boundary.\n",
      "this is multipolygon\n",
      "This is a multipolygon\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown column geometry",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'geometry'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\geopandas\\geodataframe.py:271\u001b[0m, in \u001b[0;36mGeoDataFrame.set_geometry\u001b[1;34m(self, col, drop, inplace, crs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     level \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\geopandas\\geodataframe.py:1299\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;124;03mIf the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03mGeoSeries. If it's a DataFrame with a 'geometry' column, return a\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;124;03mGeoDataFrame.\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1299\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGeoDataFrame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m geo_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_geometry_column_name\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3458\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3362\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mand\u001b[39;00m isna(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasnans:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'geometry'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m csvname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtpatterson\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive - DOI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDM_Metadatafiles\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCatalogCSV\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcatalogCSVvtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m workspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtpatterson\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive - DOI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDM_Metadatafiles\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCatalogCSV\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMBMExtentTest\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmdEditor_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataToRead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact_md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsvname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mmdEditor_read\u001b[1;34m(metadataToRead, contact_md, csvname, workspace, recordtype)\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m    \n\u001b[1;32m--> 462\u001b[0m poly \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGeoDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeoInput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeometry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPSG:4326\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#crs = lat, long designation \u001b[39;00m\n\u001b[0;32m    463\u001b[0m polyname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(workspace \u001b[38;5;241m+\u001b[39m title[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m12\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.shp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#generate name for shapefile \u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m#if geography is desired, then merge metadata with extent and output shapefile\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m#if outShape == True:\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m#Create shapefile from csv with geographic info\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m  \n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m#write shapefile\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\geopandas\\geodataframe.py:167\u001b[0m, in \u001b[0;36mGeoDataFrame.__init__\u001b[1;34m(self, geometry, crs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRS mismatch between CRS of the passed geometries \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeoDataFrame.set_crs(crs, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;66;03m# TODO: raise error in 0.9 or 0.10.\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geometry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m crs:\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssigning CRS to a GeoDataFrame without a geometry column is now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will not be supported in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    174\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    175\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\geopandas\\geodataframe.py:273\u001b[0m, in \u001b[0;36mGeoDataFrame.set_geometry\u001b[1;34m(self, col, drop, inplace, crs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     level \u001b[38;5;241m=\u001b[39m frame[col]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown column \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m col)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown column geometry"
     ]
    }
   ],
   "source": [
    "#testing for individual metadata files:\n",
    "\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\PacificFlywayWinterBrantSurvey-mdeditor-20220705-150787.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\EielsonAvianStudy-mdeditor-20220627-170675.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\SeaDuckAtlas-mdeditor-20220629-110616.json'\n",
    "##metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\LesserYellowlegEcology-mdeditor-20220705-210739.json'\n",
    "metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\mbmwa_013_Izembek_Fall_Brant_Survey-init-mdeditor-20221110.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\RedKnotStudy-mdeditor-20220627-160603.json'\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\AKContactsmdeditor-20211228-101265.json'\n",
    "\n",
    "# Pathway to csv file where to write metaata\n",
    "csvname = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\CatalogCSV\\\\\\\\catalogCSVvtest.csv'\n",
    "\n",
    "workspace = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\MBMExtentTest\\\\'\n",
    "\n",
    "mdEditor_read(metadataToRead, contact_md, csvname, workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eabb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e2817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplified extent of Sea Duck Key Habitat Extent for North America\n",
      "this is polygon\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmjv_001_SeaDuck_Key_Habitat_Atlas\\metadata\\SeaDuckAtlas-mdeditor-20220629-110616.json\n",
      "Alaska Red Knot Study 2010-Present  has NO distribution metadata\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "Alaska Red Knot Banding Data 2010-Present  has NO distribution metadata\n",
      "Alaska Red Knot Nest Data 2010-Present  has NO distribution metadata\n",
      "Alaska Red Knot Brood Data 2010-Present  has NO distribution metadata\n",
      "Alaska Red Knot Geolocator Raw Data 2006-Present  has NO distribution metadata\n",
      "Alaska Red Knot Reports 2011-Present  has NO distribution metadata\n",
      "Alaska Red knot Study Data Management Plan  has NO distribution metadata\n",
      "Alaska Red Knot Resight Data 2010-Present  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmlb_003_Red_Knot_Breeding\\metadata\\RedKnotStudy-mdeditor-20220627-160603.json\n",
      "Alaska Eielson Air Force Base Avian Study 2019-Present  has NO distribution metadata\n",
      "NotDescribed\n",
      "this is polygon\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "Alaska Eielson Air Force Base Avian Study Data Management Plan  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmlb_004_Eielson\\metadata\\EielsonAvianStudy-mdeditor-20220627-170675.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmlb_005_Lesser_Yellowleg_Ecology\\metadata\\LesserYellowlegEcology-mdeditor-20220705-210739.json\n",
      "Alaska Landbird Monitoring Survey Breeding Bird Off-Road Point Count;  2003-Present  has NO distribution metadata\n",
      "NotDescribed\n",
      "this is polygon\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "Alaska Landbird Monitoring Survey Point Count Distance Data 2006-Present   has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Point Count Summary Data 2015-Present  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Point Count Details Data 2006-Present  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Point Count Weather Data 2016-Present  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Point Count Habitat Data 2006-Present  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Observer Code Reference Data  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Point Reference Data  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Block Reference Data  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Point Count Directional Photos 2015-Present  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Point Count Protocol 2004-Present  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Summary Reports 2015-Present  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Datasheets 2004-Present  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Habitat Reference Documents  has NO distribution metadata\n",
      "Alaska Landbird Monitoring Survey Block Maps 2020  has NO distribution metadata\n",
      "Data Management Plan  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmlb_007_NWR_Alaska_Landbird_Monitoring_Survey\\metadata\\mbmlb_007_Alaska_Landbird_Monitoring_Survey-init-mdeditor-20221220.json\n",
      "Alaska Yakutat Marine Bird and Mammal Boat Survey 1996-2001  has NO distribution metadata\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "Alaska Yakutat Seabird Colony Count Boat Survey 2000  has NO distribution metadata\n",
      "Alaska Yakutat Seabird Transect Boat Survey 2000  has NO distribution metadata\n",
      "Alaska Yakutat Seabird Transect Boat Survey Locations 2000  has NO distribution metadata\n",
      "Alaska Yakutat Shorebird Point Count Boat Survey 1996-1997  has NO distribution metadata\n",
      "Alaska Yakutat Marine Bird and Mammal Boat Survey Protocol 1996-2000  has NO distribution metadata\n",
      "Spring Migration of Shorebirds on the Yakutat Forelands;  Alaska 1998  has NO distribution metadata\n",
      "Marine Bird and Mammal Survey of Yakutat Bay;  Disenchantment Bay;  Russell Fiord;  And Nunatak Fiord;  Alaska 2001  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmsb_001_YAK_Bird_Mammal_Survey\\metadata\\Yakutat_mdeditor-20221122-131177.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmsb_003_PWS_Ecological_Study_KIMU\\metadata\\PWS_KIMU-mdeditor-20220722-00987.json\n",
      "Alaska Prince William Sound Kittlitzs Murrelet Boat Survey 2001.  has NO distribution metadata\n",
      "NotDescribed\n",
      "this is polygon\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "Alaska Prince William Sound Kittlitzs Murrelet Boat Survey Data 2001  has NO distribution metadata\n",
      "Alaska Prince William Sound Kittlitzs Murrelet Boat Survey Shapefiles 2001  has NO distribution metadata\n",
      "Alaska Prince William Sound Kittlitzs Murrelet Boat Survey GeoJSON 2001  has NO distribution metadata\n",
      " Alaska Prince William Sound Kittlitzs Murrelet Boat Survey Report 2001  has NO distribution metadata\n",
      "Data Management Plan  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmsb_006_PWS_KIMU_Boat_Survey_2001\\metadata\\mbmsb_006_PWS_KIMU_Boat_Survey_2001-init-mdeditor-20221103.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmss_001_CRDevaluation\\metadata\\CRD_Goose_mdeditor-20221130-141102.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_001_YKD_Aerial_Survey\\metadata\\YKD-Aerial-Survey-mdeditor-20220629-150619.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_002_YKD_Nest_Plot_Survey\\metadata\\YKD_Nest_Plot_Survey-mdeditor-20220630-190631.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_003_WBPHS_AK\\metadata\\WBPHS_mdeditor-20220707-180736.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_004_TLSA_Molting_Goose_Survey\\metadata\\TLSAMoltingGooseSurvey-mdeditor-20220628-150611.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_006_AK_TRUS_survey\\metadata\\TrumpeterSwanSurvey_mdeditor-20220718-170793.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_007_Fall_STEI_Overhead\\metadata\\OverheadSTEI-mdeditor-20220826-090822.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_008_ACP_Aerial_Survey\\metadata\\ACP-mdeditor-20220725-210770.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_009_SEAK_Nearshore_Waterbird_Survey\\metadata\\SEAK-mdeditor-20220826-100870.json\n",
      "Alaska Izembek Brant Winter Aerial Survey 1981-present  has NO distribution metadata\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_010_Izembek_Winter_Brant_Aerial_Survey\\metadata\\IZW_mdeditor-20221115-111194.json\n",
      "Pacific Flyway Alaska Winter Brant Survey 1981-present  has NO distribution metadata\n",
      "Izembek National Wildlife Refuge, simplified boundary.\n",
      "this is polygon\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_010_Izembek_Winter_Brant_Aerial_Survey\\metadata\\PacificFlywayWinterBrantSurvey-mdeditor-20220705-150787.json\n",
      "All DONE!/n Number of contacts in master list =  407\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_011_AK_Swan_Database\\metadata\\SwanDatabase-mdeditor-20220627-140673.json\n",
      "Alasaka Izembek Fall Brant Aerial Survey 1976-Present  has NO distribution metadata\n",
      "Izembek National Wildlife Refuge, simplified boundary.\n",
      "this is multipolygon\n",
      "This is a multipolygon\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\shapely\\geometry\\multipolygon.py:189\u001b[0m, in \u001b[0;36mgeos_multipolygon_from_polygons\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mexemplar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincoming\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m root \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmdeditor\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     21\u001b[0m     jfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root,name)\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mmdEditor_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact_md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsvname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     MBMmetadataNo \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m (jfile)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mmdEditor_read\u001b[1;34m(metadataToRead, contact_md, csvname, workspace, recordtype)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m#gcoordinates = gEgeometry.get('coordinates')#list\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m#gEcoordinates = gcoordinates[0] #list\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is a multipolygon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 408\u001b[0m     mpoly_coord \u001b[38;5;241m=\u001b[39m \u001b[43mMultiPolygon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgEcoordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     geoattributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m:gEid, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:gname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m:mpoly_coord, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m:typee, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m:title, \n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maltTitle\u001b[39m\u001b[38;5;124m'\u001b[39m:altTitle, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtypename\u001b[39m\u001b[38;5;124m'\u001b[39m:typeename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurpose\u001b[39m\u001b[38;5;124m'\u001b[39m:purpose, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m:abstract, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortAb\u001b[39m\u001b[38;5;124m'\u001b[39m:shortAbstract, \n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPointOC\u001b[39m\u001b[38;5;124m'\u001b[39m:PointOC, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrustee\u001b[39m\u001b[38;5;124m'\u001b[39m:owner, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPI\u001b[39m\u001b[38;5;124m'\u001b[39m:princ, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustodian\u001b[39m\u001b[38;5;124m'\u001b[39m:custodian, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m'\u001b[39m:admin,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetaIdent\u001b[39m\u001b[38;5;124m'\u001b[39m:metaIdentifier, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetaStatus\u001b[39m\u001b[38;5;124m'\u001b[39m:metadataStatus, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m:keywords, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaxname\u001b[39m\u001b[38;5;124m'\u001b[39m:taxname,\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomname\u001b[39m\u001b[38;5;124m'\u001b[39m:comname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistrib\u001b[39m\u001b[38;5;124m'\u001b[39m:distInfoString, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massoc\u001b[39m\u001b[38;5;124m'\u001b[39m:assoclistString} \u001b[38;5;66;03m#gEid?} #creating dict of geoatttribut                        \u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\shapely\\geometry\\multipolygon.py:60\u001b[0m, in \u001b[0;36mMultiPolygon.__init__\u001b[1;34m(self, polygons, context_type)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolygons\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 60\u001b[0m     geom, n \u001b[38;5;241m=\u001b[39m \u001b[43mgeos_multipolygon_from_polygons\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolygons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeojson\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     62\u001b[0m     geom, n \u001b[38;5;241m=\u001b[39m geos_multipolygon_from_py(polygons)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\shapely\\geometry\\multipolygon.py:191\u001b[0m, in \u001b[0;36mgeos_multipolygon_from_polygons\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m    189\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(exemplar[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[43mexemplar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndim\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    195\u001b[0m subs \u001b[38;5;241m=\u001b[39m (c_void_p \u001b[38;5;241m*\u001b[39m L)()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute '_ndim'"
     ]
    }
   ],
   "source": [
    "#Search Migratory Birds Management RDR folder for mdeditor files to extract metadata\n",
    "#Count number of preserved mdEditor records\n",
    "import os\n",
    "RDR = '\\\\\\\\ifw7ro-file.fws.doi.net\\\\datamgt\\\\mbm'\n",
    "MBMmetadataNo = 0\n",
    "program = \"Migratory Bird Manangement\"\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\AKContactsmdeditor-20211228-101265.json'\n",
    "\n",
    "# Pathway to csv file where to write metaata\n",
    "csvname = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\CatalogCSV\\\\\\\\catalogCSVPhase2v1.csv'\n",
    "\n",
    "workspace = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\MBMExtentTest\\\\'\n",
    "\n",
    "#loop through RDR folder structure and find mdeditor json files that is NOT in incoming folder\n",
    "for root, dirs, files in os.walk(RDR,topdown=True):\n",
    "    #print (\"root=\", root, \"  dirs=\", dirs, \"  file=\", files)\n",
    "    for name in files:\n",
    "        if 'incoming' not in root and 'mdeditor' in name and name.endswith('.json'):\n",
    "            jfile = os.path.join(root,name)\n",
    "            print(jfile)\n",
    "            mdEditor_read(jfile, contact_md, csvname, workspace)\n",
    "            MBMmetadataNo += 1\n",
    "            print ('mdJSON read successfully')\n",
    "print (\"Number of MBM completed mdeditor records in RDR = \",MBMmetadataNo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search FES RDR folder for mdeditor files to extract metadata\n",
    "#Count number of preserved mdEditor records\n",
    "import os\n",
    "RDR = '\\\\\\\\ifw7ro-file.fws.doi.net\\\\datamgt\\\\fes'\n",
    "MBMmetadataNo = 0\n",
    "program = \"Fisheries & Ecological Services\"\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\AKContactsmdeditor-20211228-101265.json'\n",
    "\n",
    "# Pathway to csv file where to write metaata\n",
    "csvname = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\CatalogCSV\\\\\\\\catalogCSVv3.csv'\n",
    "\n",
    "#loop through RDR folder structure and find mdeditor json files that is NOT in incoming folder\n",
    "for root, dirs, files in os.walk(RDR,topdown=True):\n",
    "    #print (\"root=\", root, \"  dirs=\", dirs, \"  file=\", files)\n",
    "    for name in files:\n",
    "        if 'incoming' not in root and 'mdeditor' in name and name.endswith('.json'):\n",
    "            jfile = os.path.join(root,name)\n",
    "            mdEditor_read(jfile, contact_md, csvname)\n",
    "            MBMmetadataNo += 1\n",
    "            print (jfile)\n",
    "print (\"Number of FES completed mdeditor records in RDR = \",MBMmetadataNo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0aca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
