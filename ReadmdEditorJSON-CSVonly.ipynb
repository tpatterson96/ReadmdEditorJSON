{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e5d9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to read mdEditor files, and write out as csv \n",
    "#and shapefile with geography, if applicable\n",
    "#by Tamatha A. Patterson; verson 5; December 2022\n",
    "#distribution section updated.\n",
    "#extent merged with metadata and written to shapefile. #FeatureCollection type handled.\n",
    "#associations section updated.\n",
    "\n",
    "def mdEditor_read(metadataToRead, contact_md, csvname, workspace, recordtype = 'all'):\n",
    "    #recordtype used to write chosen resource types  \\\"all\\\", \\\"project\\\", etc.in future versions\n",
    "    #extent/geography read when is True, skipped when False.\n",
    "    import os\n",
    "    import json\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import collections\n",
    "    import fiona\n",
    "    from shapely.geometry import Point, LineString, Polygon\n",
    "    \n",
    "    def removeComma(string): #define remove comma function\n",
    "        return string.replace(\",\",\"; \")\n",
    "    \n",
    "    def listToString(s): #define converting list to a string format\n",
    "        str1 = \"; \"\n",
    "        return (str1.join(s))\n",
    "    \n",
    "    def def_value():\n",
    "        return \"none\"\n",
    "    \n",
    "    os.chdir(workspace) # assign working directory\n",
    "    \n",
    "    df = pd.read_json(metadataToRead)#read JSON metadata files into dataframe\n",
    "    values = df.get('data') #assigns metadata to values\n",
    "    for e in range(0, len(values)): #json file may have multiple metadata records\n",
    "        element = values[e] #assign list value e containing the metadata #keys= id, attributes, type\n",
    "        ID = element.get('id') #get metadata id\n",
    "        attribute = element.get('attributes') # keys= profile, json, data-updated\n",
    "        typpe = element.get('type') #get metadata type\n",
    "        if typpe != 'records':  \n",
    "            # skip if metadata is not a record and is a data dictionary, setting, schemas, custom-profiles...\n",
    "            continue  #go to next record\n",
    "\n",
    "        #parse 'attribute' to create profile, json, and date-updated\\n\",\n",
    "        dateUpdate = attribute.get('date-updated') #create dateUpdate value--where does this date come from???\n",
    "        jsondata = attribute.get('json') #create'json' data value\n",
    "        profile = attribute.get('profile') #create 'profile' value\n",
    "        \n",
    "        #convert string to dictionary\n",
    "        jsondatadict = json.loads(jsondata)  #3 keys = schema, metadata, mdDictionary\n",
    "        schema = jsondatadict.get('schema')\n",
    "        metadata = jsondatadict.get('metadata') #4 keys = metadataInfo, resourceInfo, associatedResource, resourceDistribution\\n\",\n",
    "        mdDictionary = jsondatadict.get('mdDictionary')\n",
    "        #get metadata key entries\n",
    "        metadataInfo = metadata.get('metadataInfo') #6 Keys = metadataIdentifier, metadataContact, defaultMetadataLocale, metadataDate, parentMetadata, metadataStatus\n",
    "        resourceInfo = metadata.get('resourceInfo') #12 keys = resourceType, citation, pointOfContact, abstract, shortAbstract, status, defaultResourceLocale, extent, keyword, purpose, taxonomy, timePeriod\n",
    "        associatedResource = metadata.get('associatedResource') #is list\n",
    "        resourceDistribution = metadata.get('resourceDistribution') #keys = n\n",
    "\n",
    "        #parse metadataInfo dictionary 6 keys\n",
    "        metadataIdentifier = metadataInfo.get('metadataIdentifier') #Harvest as ID to fields\n",
    "        metadataContact = metadataInfo.get('metadataContact')\n",
    "        defaultMetadataLocale = metadataInfo.get('defaultMetadataLocale')\n",
    "        metadataDate = metadataInfo.get('metadataDate')\n",
    "        parentMetadata = metadataInfo.get('parentMetadata')\n",
    "        metadataStatus = metadataInfo.get('metadataStatus') #Harvest as status to fields\n",
    "\n",
    "        #get metadata uuid identifier; autocreated in mdEditor\\n\",\n",
    "        if metadataIdentifier['namespace'] == 'urn:uuid':\n",
    "                metaIdentifier = metadataIdentifier.get('identifier')\n",
    "\n",
    "        #parse resourceInfo 12 keys: 'resourceType', 'citation', 'pointOfContact', 'abstract', 'shortAbstract', 'status', \\n\",\n",
    "        #...'defaultResourceLocale', 'extent', 'keyword', 'purpose', 'taxonomy', 'timePeriod'\\n\",\n",
    "        resourceType = resourceInfo.get('resourceType')\n",
    "        citation = resourceInfo.get('citation')\n",
    "        pointOfContact = resourceInfo.get('pointOfContact')\n",
    "        abstract = removeComma(resourceInfo.get('abstract')) #harvested to fields\n",
    "        if resourceInfo.get('shortAbstract') == None:\n",
    "            shortAbstract = \" \"\n",
    "        else:\n",
    "            shortAbstract = removeComma(resourceInfo.get('shortAbstract'))#harvested as shortAbstract to fields\n",
    "        statusList = resourceInfo.get('status')\n",
    "        status = statusList[0]#harvest as status to fields\n",
    "        defaultResourceLocale = resourceInfo.get('defaultResourceLocale')\n",
    "        extent = resourceInfo.get('extent')\n",
    "        keyword = resourceInfo.get('keyword')\n",
    "        if resourceInfo.get('purpose') == None:\n",
    "            purpose = \" \"\n",
    "        else:\n",
    "            purpose = removeComma(resourceInfo.get('purpose')) #harvested to fields\n",
    "        taxonomy = resourceInfo.get('taxonomy')\n",
    "        timePeriod = resourceInfo.get('timePeriod')\n",
    "\n",
    "        #find last update date from metadataDate\n",
    "        #Consider comparing this to last run date and only reading metadata updated after????\n",
    "        if len(metadataDate) == 1:\n",
    "            lastUpdate = metadataDate[0].get('date') \n",
    "            dateType = metadataDate[0].get('dateType')\n",
    "        else:\n",
    "            if len(metadataDate) > 1:\n",
    "                for i in metadataDate:\n",
    "                    if i.get('dateType') == \"lastUpdate\":\n",
    "                        lastUpdate = (i.get('date')).split('T')[0]\n",
    "                        dateType = 'last updated'\n",
    "                    else: \n",
    "                        dateType = i.get('dateType')\n",
    "                        lastUpdate = (i.get('date')).split('T')[0]                   \n",
    "\n",
    "        #parse resource Type info\n",
    "        typelist = resourceType[0]\n",
    "        typee = typelist.get('type')\n",
    "        typeename = typelist.get('name')\n",
    "        if typeename != None: \n",
    "            typeename = removeComma(typelist.get('name'))\n",
    "\n",
    "        #parse citation info\n",
    "        title = removeComma(citation.get('title')) #harvested as title to fields\n",
    "        dates = citation.get('date')\n",
    "        responsibleParty = citation.get('responsibleParty')\n",
    "        altTitle = citation.get('alternateTitle')\n",
    "        if altTitle != None:\n",
    "            altTitle = listToString(altTitle)#Harvested as altTitle to fields\n",
    "            altTitle = removeComma(altTitle)\n",
    "        \n",
    "        #Get and format startDate and endDate\n",
    "        try:\n",
    "            startDate = (timePeriod.get('startDateTime','None')).split('T')[0]\n",
    "            end = timePeriod.get('endDateTime', 'None')\n",
    "            if end == None:\n",
    "                endDate = 'onGoing'\n",
    "            else:\n",
    "                endDate = end.split('T')[0]\n",
    "        except:\n",
    "            startDate = 'None'\n",
    "            endDate = 'None'\n",
    "        \n",
    "        #create empty list for keywords\n",
    "        klist =[]\n",
    "        #loop through keyword thesaurus and add keywords to keyword list\n",
    "        try:\n",
    "            for g in range(0, len(keyword)):\n",
    "                word = keyword[g]\n",
    "                word1 = word.get('keyword')\n",
    "                for h in range(0, len(word1)):\n",
    "                    word2 = word1[h]\n",
    "                    word3 = word2.get('keyword')\n",
    "                    klist.append(word3)\n",
    "            keywords = listToString(klist)\n",
    "        except:\n",
    "            keywords = 'None'\n",
    "            \n",
    "        #parse species names from taxonomy; may need to loop if more than one species.\n",
    "        try: # check for taxonomy entry\n",
    "            taxdic = taxonomy[0]\n",
    "            taxClass = taxdic.get('taxonomicClassification')\n",
    "            taxSys = taxdic.get('taxonomicSystem')\n",
    "            taxvoucher = taxdic.get('voucher')\n",
    "            taxClass1 = taxClass[0]\n",
    "            taxClass1.keys()\n",
    "            taxSysID = taxClass1.get('taxonoicSystemID')\n",
    "            taxLevel = taxClass1.get('taxonomicLevel')\n",
    "            taxName = taxClass1.get('taxonomicName')\n",
    "            taxSubClass = taxClass1.get('subClassification')\n",
    "            taxIs = taxClass1.get('isITIS')\n",
    "            taxSub0 = taxSubClass[0]\n",
    "            taxSysID0 = taxSub0.get('taxonoicSystemID')\n",
    "            taxLevel0 = taxSub0.get('taxonomicLevel')\n",
    "            taxName0 = taxSub0.get('taxonomicName')\n",
    "            taxSubClass0 = taxSub0.get('subClassification')\n",
    "            taxIs0 = taxSub0.get('isITIS')\n",
    "            subKingdom =taxSubClass0[0]\n",
    "            infraKingdomL = subKingdom.get('subClassification')\n",
    "            infraKingdom = infraKingdomL[0]\n",
    "            phylumL = infraKingdom.get('subClassification')\n",
    "            phylum = phylumL[0]\n",
    "            subphylumL = phylum.get('subClassification')\n",
    "            subphylum = subphylumL[0]\n",
    "            infraphylumL = subphylum.get('subClassification')\n",
    "            infraphylum = infraphylumL[0]\n",
    "            superclassL = infraphylum.get('subClassification')\n",
    "            superclass = superclassL[0]\n",
    "            classL = superclass.get('subClassification')\n",
    "            classD = classL[0]\n",
    "            orderL = classD.get('subClassification')\n",
    "            order = orderL[0]\n",
    "            familyL = order.get('subClassification')\n",
    "            family = familyL[0]\n",
    "            genusL = family.get('subClassification')\n",
    "            genus = genusL[0]\n",
    "            taxname = genus.get('taxonomicName') #harvested to fields\n",
    "            t =[taxname]\n",
    "            comname = genus.get('commonName') #harvested to fields\n",
    "            t.append(comname)\n",
    "            comname = listToString(comname)\n",
    "            comname = removeComma(comname)\n",
    "        except:  # if no taxomony, then assign 'none'\n",
    "            taxname = \"none\"\n",
    "            comname = \"none\"  \n",
    "    \n",
    "    #Get Associated Resource Info\n",
    "    #if assocated resources are selected from metadata records.... then:\n",
    "        try:\n",
    "            assoclist = []  # create empty list of assocated resource\n",
    "            assocdic = associatedResource[0] #dict_keys(['resourceType', 'resourceCitation', 'associationType', 'initiativeType'])\n",
    "            aresourceType = assocdic.get('resourceType')\n",
    "            aresourceCitation = assocdic.get('resourceCitation')\n",
    "            aassociationType = assocdic.get('associationType')#harvest as associationType ie parentProject\n",
    "            ainitiativeType = assocdic.get('initiativeType') #initiativeType ie project\n",
    "            if aassociationType == 'parentProject' and aresourceType != None:  #this is product metadata record\n",
    "                aresourceType1 = aresourceType[0] #i.e.{'type': 'project', 'name': 'Lesser Yellowlegs Ecology'}\n",
    "                atype = aresourceType1.get('type') #get first entry to remove brackets\n",
    "                aname = aresourceType1.get('name')\n",
    "                addassoc = \"parentProject: \" + aname\n",
    "                assoclist.append(addassoc)\n",
    "            elif aassociationType == 'product': #this is project metadata record\n",
    "                for a in range (0, len(associatedResource)):\n",
    "                    l = associatedResource[a]\n",
    "                    k = l.get('mdRecordId')\n",
    "                    assoclist.append(k)\n",
    "            else:\n",
    "                assoclist = 'no assocated records'\n",
    "        except:\n",
    "            assoclist = 'no assocated records present'\n",
    "\n",
    "        #print ('associations are: ', assoclist)\n",
    "\n",
    "    \n",
    "    #get resourceDistribution metada\n",
    "        resourceDistribution = metadata.get('resourceDistribution')\n",
    "        distlist = {} #create empty distribution list\n",
    "        try:\n",
    "            for d in range(0, len(resourceDistribution)): #iterate through resourceDistribution info list\n",
    "                distributor = resourceDistribution[d]\n",
    "                dist = dict(distributor)\n",
    "                dist0 = dist.get(\"distributor\")\n",
    "                dist1 = dist0[0] #dictionary keys = 'contact', 'transferOption'\n",
    "                contact = dist1.get('contact')\n",
    "                order = dist1.get('orderProcess')\n",
    "                transopt = dist1.get('transferOption')\n",
    "                distrole =contact.get('role') #harvested as distributor role to distlist\n",
    "                distparty = contact.get('party') #distrbutor contact identifiers\n",
    "                #if len(distparty) > 1:\n",
    "                    #for org in range(0,len(distparty)): \n",
    "                        #distID = distparty[0]\n",
    "                        #distributorID = distID.get('contactId') #harvest distributor ID & compare with contact master list be\n",
    "                #else:\n",
    "                distID = distparty[0]\n",
    "                distributorID = distID.get('contactId') #harvest distributor ID & compare with contact master list be\n",
    "                transopt1 = transopt[0]\n",
    "                transopt2 = transopt1.get('onlineOption')\n",
    "                transopt3 = transopt2[0]\n",
    "                onlineName = transopt3.get('name') #harvested to distlist\n",
    "                onlineUri = transopt3.get('uri') #harvested to distlist\n",
    "                distInfo =[distrole, distributorID, onlineName, onlineUri]\n",
    "                distlist[d] = distInfo\n",
    "                #print (\"distlist = \", distlist)\n",
    "                distInfoString = '; '.join(distInfo)\n",
    "            \n",
    "        except:\n",
    "            print(title, ' has NO distribution metadata')\n",
    "            distInfoString = ' '\n",
    "      \n",
    "    #POINTS of CONTACT\n",
    "        #read Master Contact JSON metadata file into dataframe\n",
    "        contactmetadata = pd.read_json(contact_md)\n",
    "        contactmd1 = dict(contactmetadata)\n",
    "        contactmd2 = contactmd1.get('data')\n",
    "        \n",
    "        POC = collections.defaultdict(list) # create empty dictionary for contacts\n",
    "        POCvalues = []\n",
    "        count = 0\n",
    "\n",
    "        #iterate through master contact metadata\n",
    "        for k in contactmd2:\n",
    "            contactmd3 = contactmd2[count]\n",
    "            contactmd4 = contactmd3.get('attributes')\n",
    "            contactmd5 = contactmd4.get('json')\n",
    "            contactmd6 = json.loads(contactmd5)\n",
    "            contactmd7 = dict(contactmd6)\n",
    "            contactIDmd = contactmd7.get('contactId') #harvest id#\n",
    "            count += 1\n",
    "\n",
    "            for id in distlist:\n",
    "                if distlist[id][1] == contactIDmd:\n",
    "                    contactisOrganizationmd = contactmd7.get('isOrganization')\n",
    "                    contactName = contactmd7.get('name') #havest as distributor name to fields\n",
    "                    contactMemberOf = contactmd7.get('memberOfOrganization')\n",
    "                    contactemail = contactmd7.get('electronicMailAdddress')\n",
    "                    contactType = contactmd7.get('contactType')\n",
    "                    distlist[id][1] = contactName \n",
    "\n",
    "            # iterate through contacts from metadata\n",
    "            for j in pointOfContact:\n",
    "                party = j.get('party')\n",
    "                for p in range(0, len(party)):\n",
    "                    partyContact = party[p]\n",
    "                    partyContactID = partyContact.get('contactId') #id to compare in master contact list\n",
    "                    role = j.get('role')\n",
    "\n",
    "                    #compare master list contact ID with metadata contact ID\n",
    "                    if contactIDmd == partyContactID:\n",
    "                        contactisOrganizationmd = contactmd7.get('isOrganization')\n",
    "                        contactName = contactmd7.get('name')\n",
    "                        contactMemberOf = contactmd7.get('memberOfOrganization')\n",
    "                        contactemail = contactmd7.get('electronicMailAdddress')\n",
    "                        contactType = contactmd7.get('contactType')\n",
    "                        POC[role].append(contactName)\n",
    "                    #else:\n",
    "                        continue\n",
    "\n",
    "        owner = listToString(POC['owner'])\n",
    "        PointOC = listToString(POC['pointOfContact'])\n",
    "        princ = listToString(POC['principalInvestigator'])\n",
    "        custodian = listToString(POC['custodian'])\n",
    "        admin = listToString(POC['administrator'])\n",
    "        originator = listToString(POC['originator'])\n",
    "        contributor = listToString(POC['contributor'])\n",
    "        #distlistString = '; '.join(distInfo) \n",
    "\n",
    "   #Write vales to CSV\n",
    "        fields = [ID, typee, title, altTitle, typeename, purpose, abstract, shortAbstract, \n",
    "                  PointOC, owner, princ, custodian, admin, originator, contributor, startDate, endDate, lastUpdate, status, \n",
    "                  metaIdentifier, metadataStatus, keywords, taxname, comname, distInfoString, assoclist] #gEid?\n",
    "\n",
    "        #write files to csv.\n",
    "        with open (csvname, 'a', newline = '') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(fields)     \n",
    "        \n",
    "    return\n",
    "    print(\"ALL DONE! CSV created./n\",\"Number of contacts in master list = \", len(contactmd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59db6d17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alasaka Izembek Fall Brant Aerial Survey 1976-Present  has NO distribution metadata\n",
      "this is a feature collection extent\n",
      "Izembek National Wildlife Refuge, simplified boundary.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown column geometry",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'geometry'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\geopandas\\geodataframe.py:271\u001b[0m, in \u001b[0;36mGeoDataFrame.set_geometry\u001b[1;34m(self, col, drop, inplace, crs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     level \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\geopandas\\geodataframe.py:1299\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;124;03mIf the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03mGeoSeries. If it's a DataFrame with a 'geometry' column, return a\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;124;03mGeoDataFrame.\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1299\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGeoDataFrame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m geo_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_geometry_column_name\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3458\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3362\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mand\u001b[39;00m isna(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasnans:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'geometry'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m csvname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtpatterson\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive - DOI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDM_Metadatafiles\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCatalogCSV\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcatalogCSVvtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m workspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtpatterson\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive - DOI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDM_Metadatafiles\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCatalogCSV\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmdEditor_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataToRead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact_md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsvname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36mmdEditor_read\u001b[1;34m(metadataToRead, contact_md, csvname, workspace, recordtype)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m    \n\u001b[1;32m--> 416\u001b[0m poly \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGeoDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeoInput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeometry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPSG:4326\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#crs = lat, long designation \u001b[39;00m\n\u001b[0;32m    417\u001b[0m polyname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(ID \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.shp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#generate unique name for shapefile \u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m#if geography is desired, then merge metadata with extent and output shapefile\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m#if outShape == True:\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m#Create shapefile from csv with geographic info\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\geopandas\\geodataframe.py:167\u001b[0m, in \u001b[0;36mGeoDataFrame.__init__\u001b[1;34m(self, geometry, crs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRS mismatch between CRS of the passed geometries \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeoDataFrame.set_crs(crs, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;66;03m# TODO: raise error in 0.9 or 0.10.\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geometry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m crs:\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssigning CRS to a GeoDataFrame without a geometry column is now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will not be supported in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    174\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    175\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\geopandas\\geodataframe.py:273\u001b[0m, in \u001b[0;36mGeoDataFrame.set_geometry\u001b[1;34m(self, col, drop, inplace, crs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     level \u001b[38;5;241m=\u001b[39m frame[col]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown column \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m col)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown column geometry"
     ]
    }
   ],
   "source": [
    "#testing for individual metadata files:\n",
    "\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\PacificFlywayWinterBrantSurvey-mdeditor-20220705-150787.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\EielsonAvianStudy-mdeditor-20220627-170675.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\SeaDuckAtlas-mdeditor-20220629-110616.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\LesserYellowlegEcology-mdeditor-20220705-210739.json'\n",
    "metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\mbmwa_013_Izembek_Fall_Brant_Survey-init-mdeditor-20221110.json'\n",
    "#metadataToRead = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\RedKnotStudy-mdeditor-20220627-160603.json'\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\AKContactsmdeditor-20211228-101265.json'\n",
    "\n",
    "# Pathway to csv file where to write metaata\n",
    "csvname = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\CatalogCSV\\\\\\\\catalogCSVvtest.csv'\n",
    "\n",
    "workspace = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV'\n",
    "\n",
    "mdEditor_read(metadataToRead, contact_md, csvname, workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eabb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9e2817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmjv_001_SeaDuck_Key_Habitat_Atlas\\metadata\\SeaDuckAtlas-mdeditor-20220629-110616.json\n",
      "Alaska Red Knot Study 2010-Present  has NO distribution metadata\n",
      "Alaska Red Knot Banding Data 2010-Present  has NO distribution metadata\n",
      "Alaska Red Knot Nest Data 2010-Present  has NO distribution metadata\n",
      "Alaska Red Knot Brood Data 2010-Present  has NO distribution metadata\n",
      "Alaska Red Knot Geolocator Raw Data 2006-Present  has NO distribution metadata\n",
      "Alaska Red Knot Reports 2011-Present  has NO distribution metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alaska Red knot Study Data Management Plan  has NO distribution metadata\n",
      "Alaska Red Knot Resight Data 2010-Present  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmlb_003_Red_Knot_Breeding\\metadata\\RedKnotStudy-mdeditor-20220627-160603.json\n",
      "Alaska Eielson Air Force Base Avian Study 2019-Present  has NO distribution metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alaska Eielson Air Force Base Avian Study Data Management Plan  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmlb_004_Eielson\\metadata\\EielsonAvianStudy-mdeditor-20220627-170675.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmlb_005_Lesser_Yellowleg_Ecology\\metadata\\LesserYellowlegEcology-mdeditor-20220705-210739.json\n",
      "Alaska Yakutat Marine Bird and Mammal Boat Survey 1996-2001  has NO distribution metadata\n",
      "Alaska Yakutat Seabird Colony Count Boat Survey 2000  has NO distribution metadata\n",
      "Alaska Yakutat Seabird Transect Boat Survey 2000  has NO distribution metadata\n",
      "Alaska Yakutat Seabird Transect Boat Survey Locations 2000  has NO distribution metadata\n",
      "Alaska Yakutat Shorebird Point Count Boat Survey 1996-1997  has NO distribution metadata\n",
      "Alaska Yakutat Marine Bird and Mammal Boat Survey Protocol 1996-2000  has NO distribution metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spring Migration of Shorebirds on the Yakutat Forelands;  Alaska 1998  has NO distribution metadata\n",
      "Marine Bird and Mammal Survey of Yakutat Bay;  Disenchantment Bay;  Russell Fiord;  And Nunatak Fiord;  Alaska 2001  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmsb_001_YAK_Bird_Mammal_Survey\\metadata\\Yakutat_mdeditor-20221122-131177.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n",
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n",
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmsb_003_PWS_Ecological_Study_KIMU\\metadata\\PWS_KIMU-mdeditor-20220722-00987.json\n",
      "Alaska Prince William Sound Kittlitz’s Murrelet Boat Survey 2001.  has NO distribution metadata\n",
      "Alaska Prince William Sound Kittlitz’s Murrelet Boat Survey Data 2001  has NO distribution metadata\n",
      "Alaska Prince William Sound Kittlitz’s Murrelet Boat Survey Shapefiles 2001  has NO distribution metadata\n",
      "Alaska Prince William Sound Kittlitz’s Murrelet Boat Survey GeoJSON 2001  has NO distribution metadata\n",
      " Alaska Prince William Sound Kittlitz’s Murrelet Boat Survey Report 2001  has NO distribution metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Management Plan  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmsb_006_PWS_KIMU_Boat_Survey_2001\\metadata\\mbmsb_006_PWS_KIMU_Boat_Survey_2001-init-mdeditor-20221103.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmss_001_CRDevaluation\\metadata\\CRD_Goose_mdeditor-20221130-141102.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_001_YKD_Aerial_Survey\\metadata\\YKD-Aerial-Survey-mdeditor-20220629-150619.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n",
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n",
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_002_YKD_Nest_Plot_Survey\\metadata\\YKD_Nest_Plot_Survey-mdeditor-20220630-190631.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_003_WBPHS_AK\\metadata\\WBPHS_mdeditor-20220707-180736.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_004_TLSA_Molting_Goose_Survey\\metadata\\TLSAMoltingGooseSurvey-mdeditor-20220628-150611.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_006_AK_TRUS_survey\\metadata\\TrumpeterSwanSurvey_mdeditor-20220718-170793.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_007_Fall_STEI_Overhead\\metadata\\OverheadSTEI-mdeditor-20220826-090822.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_008_ACP_Aerial_Survey\\metadata\\ACP-mdeditor-20220725-210770.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_009_SEAK_Nearshore_Waterbird_Survey\\metadata\\SEAK-mdeditor-20220826-100870.json\n",
      "Alaska Izembek Brant Winter Aerial Survey 1981-present  has NO distribution metadata\n",
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_010_Izembek_Winter_Brant_Aerial_Survey\\metadata\\IZW_mdeditor-20221115-111194.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacific Flyway Alaska Winter Brant Survey 1981-present  has NO distribution metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_010_Izembek_Winter_Brant_Aerial_Survey\\metadata\\PacificFlywayWinterBrantSurvey-mdeditor-20220705-150787.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpatterson\\AppData\\Local\\Temp\\1\\ipykernel_2668\\3647770736.py:403: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shape = dfmerge.to_file(polyname, encoding = 'utf-8')#, driver = 'ESRI Shapefile', schema = {\"geometry\": \"Polygon\", \"properties\":{\"id\":\"int\"}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\ifw7ro-file.fws.doi.net\\datamgt\\mbm\\mbmwa_011_AK_Swan_Database\\metadata\\SwanDatabase-mdeditor-20220627-140673.json\n",
      "Alasaka Izembek Fall Brant Aerial Survey 1976-Present  has NO distribution metadata\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A LinearRing must have at least 3 coordinate tuples",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\shapely\\speedups\\_speedups.pyx:252\u001b[0m, in \u001b[0;36mshapely.speedups._speedups.geos_linearring_from_py\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute '__array_interface__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincoming\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m root \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmdeditor\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     21\u001b[0m     jfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root,name)\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mmdEditor_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact_md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsvname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     MBMmetadataNo \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m (jfile)\n",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36mmdEditor_read\u001b[1;34m(metadataToRead, contact_md, csvname, workspace, recordtype)\u001b[0m\n\u001b[0;32m    350\u001b[0m gcoordinates \u001b[38;5;241m=\u001b[39m gEgeometry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#list\u001b[39;00m\n\u001b[0;32m    351\u001b[0m gEcoordinates \u001b[38;5;241m=\u001b[39m gcoordinates[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#list\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m poly_coord \u001b[38;5;241m=\u001b[39m \u001b[43mPolygon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgEcoordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    354\u001b[0m     gEproperties \u001b[38;5;241m=\u001b[39m nextstep\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\shapely\\geometry\\polygon.py:261\u001b[0m, in \u001b[0;36mPolygon.__init__\u001b[1;34m(self, shell, holes)\u001b[0m\n\u001b[0;32m    258\u001b[0m BaseGeometry\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shell \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 261\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mgeos_polygon_from_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m         geom, n \u001b[38;5;241m=\u001b[39m ret\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\shapely\\geometry\\polygon.py:539\u001b[0m, in \u001b[0;36mgeos_polygon_from_py\u001b[1;34m(shell, holes)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m geos_geom_from_py(shell)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shell \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 539\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mgeos_linearring_from_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\GISpy\\lib\\site-packages\\shapely\\speedups\\_speedups.pyx:346\u001b[0m, in \u001b[0;36mshapely.speedups._speedups.geos_linearring_from_py\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A LinearRing must have at least 3 coordinate tuples"
     ]
    }
   ],
   "source": [
    "#Search Migratory Birds Management RDR folder for mdeditor files to extract metadata\n",
    "#Count number of preserved mdEditor records\n",
    "import os\n",
    "RDR = '\\\\\\\\ifw7ro-file.fws.doi.net\\\\datamgt\\\\mbm'\n",
    "MBMmetadataNo = 0\n",
    "program = \"Migratory Bird Manangement\"\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\AKContactsmdeditor-20211228-101265.json'\n",
    "\n",
    "# Pathway to csv file where to write metaata\n",
    "csvname = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\CatalogCSV\\\\\\\\catalogCSVPhase2v1.csv'\n",
    "\n",
    "workspace = 'C:\\\\Users\\\\tpatterson\\\\OneDrive - DOI\\\\Documents\\\\DM_Metadatafiles\\\\CatalogCSV\\\\MBMExtentTest'\n",
    "\n",
    "#loop through RDR folder structure and find mdeditor json files that is NOT in incoming folder\n",
    "for root, dirs, files in os.walk(RDR,topdown=True):\n",
    "    #print (\"root=\", root, \"  dirs=\", dirs, \"  file=\", files)\n",
    "    for name in files:\n",
    "        if 'incoming' not in root and 'mdeditor' in name and name.endswith('.json'):\n",
    "            jfile = os.path.join(root,name)\n",
    "            mdEditor_read(jfile, contact_md, csvname, workspace)\n",
    "            MBMmetadataNo += 1\n",
    "            print (jfile)\n",
    "print (\"Number of MBM completed mdeditor records in RDR = \",MBMmetadataNo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search FES RDR folder for mdeditor files to extract metadata\n",
    "#Count number of preserved mdEditor records\n",
    "import os\n",
    "RDR = '\\\\\\\\ifw7ro-file.fws.doi.net\\\\datamgt\\\\fes'\n",
    "MBMmetadataNo = 0\n",
    "program = \"Fisheries & Ecological Services\"\n",
    "\n",
    "# Pathway to the contacts file you want to use to check against existing vs. new contacts; i.e., master AK contacts file\\n\",\n",
    "contact_md = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\AKContactsmdeditor-20211228-101265.json'\n",
    "\n",
    "# Pathway to csv file where to write metaata\n",
    "csvname = 'C:\\\\\\\\Users\\\\\\\\tpatterson\\\\\\\\OneDrive - DOI\\\\\\\\Documents\\\\\\\\DM_Metadatafiles\\\\\\\\CatalogCSV\\\\\\\\catalogCSVv3.csv'\n",
    "\n",
    "#loop through RDR folder structure and find mdeditor json files that is NOT in incoming folder\n",
    "for root, dirs, files in os.walk(RDR,topdown=True):\n",
    "    #print (\"root=\", root, \"  dirs=\", dirs, \"  file=\", files)\n",
    "    for name in files:\n",
    "        if 'incoming' not in root and 'mdeditor' in name and name.endswith('.json'):\n",
    "            jfile = os.path.join(root,name)\n",
    "            mdEditor_read(jfile, contact_md, csvname)\n",
    "            MBMmetadataNo += 1\n",
    "            print (jfile)\n",
    "print (\"Number of FES completed mdeditor records in RDR = \",MBMmetadataNo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0aca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
